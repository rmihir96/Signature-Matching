{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import csv\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C_Lambda = 0.01 #Regularization constant\n",
    "TrainingPercent = 80\n",
    "ValidationPercent = 10\n",
    "TestPercent = 10\n",
    "M = 6 #number of basis functions decided based on Kmeans clustering.\n",
    "PHI = []\n",
    "IsSynthetic = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Human Observed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Human_Observe_Dataset():\n",
    "    \n",
    "    d = []\n",
    "    with open('HumanObserved-Features-Data.csv') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            d.append(row)\n",
    "\n",
    "    dataset = {l['img_id']: [v for k, v in l.items() if k not in ('', 'img_id')] for l in d}\n",
    "    #print(dataset)   \n",
    "    samepairs = []\n",
    "    SP_target = []\n",
    "    with open('same_pairs.csv') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            samepairs.append(row[:2])\n",
    "            SP_target.append(row[2])\n",
    "        #print(samepairs)\n",
    "        #print(SP_target)\n",
    "        for i in range(len(samepairs)):\n",
    "            for k,v in dataset.items():\n",
    "                \n",
    "               # print(k,v)\n",
    "                if(samepairs[i][0])==k:\n",
    "                    samepairs[i].append(v)\n",
    "                if(samepairs[i][1])==k:\n",
    "                    samepairs[i].append(v)\n",
    "        #print(samepairs)\n",
    "    #print(len(samepairs))\n",
    "\n",
    "    d=[]\n",
    "    for i in range(len(samepairs)-1): #792 rows\n",
    "        for j in range(len(samepairs[1])-2): #Add first 3 columns directly.\n",
    "            #print(i+1,j)\n",
    "            #print(a[i+1][j])\n",
    "            d.append(samepairs[i+1][j])\n",
    "        for k in range(9):\n",
    "                #print(samepairs[i+1][3][k])\n",
    "                #print(\"------\")\n",
    "\n",
    "                d.append(int(samepairs[i+1][2][k]))\n",
    "        for k in range(9):\n",
    "            #print(samepairs[i+1][4][k])\n",
    "            d.append(int(samepairs[i+1][3][k]))\n",
    "    #print(d)\n",
    "\n",
    "    e=[]\n",
    "    for i in range(len(samepairs)-1): #792 rows\n",
    "        for j in range(len(samepairs[1])-2): #Add first 3 columns directly.\n",
    "            #print(i+1,j)\n",
    "            #print(a[i+1][j])\n",
    "            e.append(samepairs[i+1][j])\n",
    "        for k in range(9):\n",
    "                #print(samepairs[i+1][3][k])\n",
    "                #print(\"------\")\n",
    "\n",
    "                e.append(abs(int(samepairs[i+1][2][k])-int(samepairs[i+1][3][k])))\n",
    "\n",
    "    #print(e)\n",
    "\n",
    "\n",
    "    SP = np.array(d).reshape(791,20)\n",
    "    #print(SP)\n",
    "\n",
    "    #print((SP[0]))\n",
    "\n",
    "    Sub_SP = np.array(e).reshape(791,11)\n",
    "    #print(Sub_SP)\n",
    "\n",
    "    diff_pairs = pd.DataFrame(pd.read_csv(\"diffn_pairs.csv\"))\n",
    "    diff_pairs.tail()\n",
    "    diff_pairs.describe()\n",
    "\n",
    "    different_pairs = []\n",
    "    DP_target=[]\n",
    "    with open('diffn_pairs.csv') as csvfile:\n",
    "        diff_reader = csv.reader(csvfile)\n",
    "        for row in diff_reader:\n",
    "            different_pairs.append(row[:2])\n",
    "            DP_target.append(row[2])\n",
    "    #print(len(different_pairs))\n",
    "    #print(len(DP_target))\n",
    "\n",
    "    #print((different_pairs[293032]))\n",
    "\n",
    "    for i in range(len(different_pairs)):\n",
    "        for k,v in dataset.items():\n",
    "           # print(k,v)\n",
    "            if(different_pairs[i][0])==k:\n",
    "                different_pairs[i].append(v)\n",
    "            if(different_pairs[i][1])==k:\n",
    "                different_pairs[i].append(v)\n",
    "    #print(different_pairs)\n",
    "    #print(len(different_pairs))\n",
    "\n",
    "    #print((different_pairs[1]))\n",
    "\n",
    "    c=[]\n",
    "    for i in range(len(different_pairs)-1): #293031 rows\n",
    "        for j in range(len(different_pairs[1])-2): #Add first 3 columns directly.\n",
    "            #print(i+1,j)\n",
    "            #print(a[i+1][j])\n",
    "            c.append(different_pairs[i+1][j])\n",
    "        for k in range(9):\n",
    "                #print(samepairs[i+1][3][k])\n",
    "                #print(\"------\")\n",
    "\n",
    "                c.append(int(different_pairs[i+1][2][k]))\n",
    "        for k in range(9):\n",
    "            #print(samepairs[i+1][4][k])\n",
    "            c.append(int(different_pairs[i+1][3][k]))\n",
    "    #print(c)\n",
    "\n",
    "    f=[]\n",
    "    for i in range(len(different_pairs)-1): #792 rows\n",
    "        for j in range(len(samepairs[1])-2): #Add first 3 columns directly.\n",
    "            #print(i+1,j)\n",
    "            #print(a[i+1][j])\n",
    "            f.append(different_pairs[i+1][j])\n",
    "        for k in range(9):\n",
    "                #print(samepairs[i+1][3][k])\n",
    "                #print(\"------\")\n",
    "\n",
    "                f.append(abs(int(different_pairs[i+1][2][k])-int(different_pairs[i+1][3][k])))\n",
    "\n",
    "    print(f)\n",
    "\n",
    "    Diff_pairs = np.array(c).reshape(293032,20)\n",
    "    DP = Diff_pairs[0:791,]\n",
    "    Unseen_DP = Diff_pairs[791:,]\n",
    "    DP.shape\n",
    "\n",
    "    Sub_DP = np.array(f).reshape(293032,11)\n",
    "    Sub_DP = Sub_DP[0:791,]\n",
    "    Sub_DP.shape\n",
    "\n",
    "    Human_Observed_Dataset = np.concatenate((SP,DP), axis = 0)\n",
    "    Human_Observed_Dataset.shape\n",
    "    HODS = Human_Observed_Dataset[:, 2:]\n",
    "    HODS.shape\n",
    "\n",
    "    Sub_Human_Observed_Dataset = np.concatenate((Sub_SP,Sub_DP), axis = 0)\n",
    "    Sub_Human_Observed_Dataset.shape\n",
    "\n",
    "    #print(time.time()-start)\n",
    "    SHODS = Sub_Human_Observed_Dataset[:, 2:]\n",
    "        \n",
    "        \n",
    "    target = np.concatenate((np.array(SP_target[1:]), np.array(DP_target[1:792])), axis=0)\n",
    "    t = target.reshape(-1,1)\n",
    "    #print(t.shape)\n",
    "    HODS.shape\n",
    "    Final_HODS = np.concatenate((HODS, t), axis=1)\n",
    "   # print(Final_HODS.shape)\n",
    "    #print(Final_HODS)\n",
    "    Final_SHODS = np.concatenate((SHODS, t), axis=1)\n",
    "    Final_SHODS.shape\n",
    "\n",
    "    #Convert str to int\n",
    "    temp = Final_HODS.reshape(-1)\n",
    "    temp.shape\n",
    "    List = temp.tolist()\n",
    "    List = [int(x) for x in List]\n",
    "    Final_HODS = np.array(List).reshape(1582,19)\n",
    "    temp1 = Final_SHODS.reshape(-1)\n",
    "    temp1.shape\n",
    "    List = temp1.tolist()\n",
    "    List = [int(x) for x in List]\n",
    "    Final_SHODS = np.array(List).reshape(1582,10)\n",
    "    #print(Final_SHODS.dtype)\n",
    "    print(\"Human Observed Dataset Created\")\n",
    "    return Final_HODS, Final_SHODS\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Select_HODS(Final_HODS, Final_SHODS, Concatenated = True):  #Select the dataset as per concatenation/subtraction\n",
    "      if Concatenated == True:  \n",
    "        print(\"Human Observed Dataset with Concatenation selected\")\n",
    "        np.random.shuffle(Final_HODS)\n",
    "        #print(Final_HODS)\n",
    "        training_data = Final_HODS[:, :18]\n",
    "        print(training_data.shape)\n",
    "        target_data = Final_HODS[:, 18:]\n",
    "        #print(target_data.shape)\n",
    "        #print(len(training_data[0]))\n",
    "        RawData = np.transpose(training_data)\n",
    "        RawTarget = target_data\n",
    "        return RawData, RawTarget\n",
    "\n",
    "      else:\n",
    "            print(\"Human Observed Dataset with Subtraction selected\")\n",
    "            np.random.shuffle(Final_SHODS)\n",
    "            #print(Final_SHODS)\n",
    "            training_data = Final_SHODS[:, :9]\n",
    "            #print(training_data.shape)\n",
    "            target_data = Final_SHODS[:, 9:]\n",
    "            #print(target_data.shape)\n",
    "            #print(len(training_data[0]))\n",
    "            RawData = np.transpose(training_data)\n",
    "            RawTarget = target_data\n",
    "            #print(RawData.shape)\n",
    "            return RawData, RawTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_Data_Split(RawData, RawTarget): #Split the data into Training, Validation, Testing\n",
    "    TrainingTarget = (GenerateTrainingTarget(RawTarget,TrainingPercent).reshape(-1))\n",
    "    TrainingData   = GenerateTrainingDataMatrix(RawData,TrainingPercent)\n",
    "    #print(TrainingTarget.shape)\n",
    "    #print(TrainingData.shape)\n",
    "\n",
    "    ValDataAct = (GenerateValTargetVector(RawTarget,ValidationPercent, (len(TrainingTarget))).reshape(-1))\n",
    "    ValData    = GenerateValData(RawData,ValidationPercent, (len(TrainingTarget)))\n",
    "    #print(ValDataAct.shape)\n",
    "    #print(ValData.shape)\n",
    "\n",
    "    TestDataAct = (GenerateValTargetVector(RawTarget,TestPercent, (len(TrainingTarget)+len(ValDataAct))).reshape(-1))\n",
    "    TestData = GenerateValData(RawData,TestPercent, (len(TrainingTarget)+len(ValDataAct)))\n",
    "    #print(ValDataAct.shape)\n",
    "    #print(ValData.shape)\n",
    "\n",
    "    return TrainingData, TrainingTarget, ValData, ValDataAct, TestData, TestDataAct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HUMAN OBSERVED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Observed Dataset Created\n"
     ]
    }
   ],
   "source": [
    "#Create Human Observed Dataset\n",
    "Final_HODS, Final_SHODS = Create_Human_Observe_Dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSC DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gsc_dataset():\n",
    "    d = []\n",
    "    with open('GSC-Features.csv') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            d.append(row)\n",
    "\n",
    "    dataset = {l['img_id']: [v for k, v in l.items() if k not in ('', 'img_id')] for l in d}\n",
    "    #print(dataset)   \n",
    "\n",
    "    #for k,v in dataset.items():\n",
    "     #   print(k,v)\n",
    "\n",
    "    same_pairs = pd.DataFrame(pd.read_csv(\"GSC-Features.csv\"))\n",
    "    same_pairs.describe()\n",
    "\n",
    "    GSC_samepairs = []\n",
    "    GSC_SP_target = []\n",
    "    with open('GSC_same_pairs.csv') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            GSC_samepairs.append(row[:2])\n",
    "            GSC_SP_target.append(row[2])\n",
    "       # print(len(GSC_samepairs))\n",
    "        #print(len(GSC_SP_target))\n",
    "\n",
    "    #for i in range(len(samepairs)):\n",
    "        #print(samepairs[i+1])\n",
    "    #print(GSC_samepairs[1])\n",
    "\n",
    "    for i in range(5000):\n",
    "        for k,v in dataset.items():\n",
    "            #print(k,v)\n",
    "            if(GSC_samepairs[i][0])==k:\n",
    "                GSC_samepairs[i].append(v)\n",
    "            if(GSC_samepairs[i][1])==k:\n",
    "                GSC_samepairs[i].append(v)\n",
    "    #print(samepairs)\n",
    "    print(\"length of gsc same pairs\",len(GSC_samepairs[0:5000]))\n",
    "\n",
    "    #print((GSC_samepairs[1]))\n",
    "\n",
    "\n",
    "    d=[]\n",
    "    for i in range(4999): #792 rows\n",
    "        for j in range(len(GSC_samepairs[1])-2): #Add first 2 columns directly.\n",
    "            #print(i+1,j)\n",
    "            #print(a[i+1][j])\n",
    "            d.append(GSC_samepairs[i+1][j])\n",
    "        for k in range(512):\n",
    "                #print(samepairs[i+1][3][k])\n",
    "                #print(\"------\")\n",
    "\n",
    "                d.append(int(GSC_samepairs[i+1][2][k]))\n",
    "        for k in range(512):\n",
    "            #print(samepairs[i+1][4][k])\n",
    "            d.append(int(GSC_samepairs[i+1][3][k]))\n",
    "    #print(\"d\",len(d))\n",
    "\n",
    "    e=[]\n",
    "    for i in range(4999): #792 rows\n",
    "        for j in range(len(GSC_samepairs[1])-2): #Add first 3 columns directly.\n",
    "            #print(i+1,j)\n",
    "            #print(a[i+1][j])\n",
    "            e.append(GSC_samepairs[i+1][j])\n",
    "        for k in range(512):\n",
    "                #print(samepairs[i+1][3][k])\n",
    "                #print(\"------\")\n",
    "\n",
    "                e.append(abs(int(GSC_samepairs[i+1][2][k])-int(GSC_samepairs[i+1][3][k])))\n",
    "\n",
    "    #print(e)\n",
    "\n",
    "    import numpy as np\n",
    "    GSC_SP = np.array(d).reshape(4999,1026)\n",
    "    print(\"GSC_SP\",GSC_SP.shape)\n",
    "\n",
    "    #print((GSC_SP[0]))\n",
    "\n",
    "    GSC_Sub_SP = np.array(e).reshape(4999,514)\n",
    "    print(\"GSC_SUB_SP\",GSC_Sub_SP.shape)\n",
    "\n",
    "    diff_pairs = pd.DataFrame(pd.read_csv(\"GSC_diffn_pairs.csv\"))\n",
    "    diff_pairs.tail()\n",
    "    diff_pairs.describe()\n",
    "\n",
    "    GSC_different_pairs = []\n",
    "    GSC_DP_target=[]\n",
    "    with open('GSC_diffn_pairs.csv') as csvfile:\n",
    "        diff_reader = csv.reader(csvfile)\n",
    "        for row in diff_reader:\n",
    "            GSC_different_pairs.append(row[:2])\n",
    "            GSC_DP_target.append(row[2])\n",
    "    #print(len(GSC_different_pairs))\n",
    "    #print(len(GSC_DP_target))\n",
    "\n",
    "    #print((GSC_different_pairs[1]))\n",
    "\n",
    "    for i in range(5000):\n",
    "        for k,v in dataset.items():\n",
    "           # print(k,v)\n",
    "            if(GSC_different_pairs[i][0])==k:\n",
    "                GSC_different_pairs[i].append(v)\n",
    "            if(GSC_different_pairs[i][1])==k:\n",
    "                GSC_different_pairs[i].append(v)\n",
    "    #print(different_pairs)\n",
    "    #print(len(different_pairs[0:1000]))\n",
    "\n",
    "\n",
    "\n",
    "    c=[]\n",
    "    for i in range(4999): #293031 rows\n",
    "        for j in range(len(GSC_different_pairs[1])-2): #Add first 3 columns directly.\n",
    "            #print(i+1,j)\n",
    "            #print(a[i+1][j])\n",
    "            c.append(GSC_different_pairs[i+1][j])\n",
    "        for k in range(512):\n",
    "                #print(samepairs[i+1][3][k])\n",
    "                #print(\"------\")\n",
    "\n",
    "                c.append(int(GSC_different_pairs[i+1][2][k]))\n",
    "        for k in range(512):\n",
    "            #print(samepairs[i+1][4][k])\n",
    "            c.append(int(GSC_different_pairs[i+1][3][k]))\n",
    "    #print(c)\n",
    "\n",
    "    #print(\"c\",len(c))\n",
    "\n",
    "    f=[]\n",
    "    for i in range(4999): #792 rows\n",
    "        for j in range(len(GSC_different_pairs[1])-2): #Add first 3 columns directly.\n",
    "            #print(i+1,j)\n",
    "            #print(a[i+1][j])\n",
    "            f.append(GSC_different_pairs[i+1][j])\n",
    "        for k in range(512):\n",
    "                #print(samepairs[i+1][3][k])\n",
    "                #print(\"------\")\n",
    "\n",
    "                f.append(abs(int(GSC_different_pairs[i+1][2][k])-int(GSC_different_pairs[i+1][3][k])))\n",
    "\n",
    "    #print(\"f\", len(f))\n",
    "\n",
    "    GSC_DP = np.array(c).reshape(4999,1026)\n",
    "    print(\"GSC_DP\",GSC_DP.shape)\n",
    "\n",
    "    GSC_Sub_DP = np.array(f).reshape(4999,514)\n",
    "    print(\"GSC_SUB_DP\",GSC_Sub_DP.shape)\n",
    "\n",
    "    GSC_Dataset = np.concatenate((GSC_SP[:2500],GSC_DP[:2500]), axis = 0)\n",
    "    GSC_Dataset = GSC_Dataset[:, 2:]\n",
    "    print(\"GSC_Dataset\", GSC_Dataset.shape)\n",
    "\n",
    "    Unseen_GSC = np.concatenate((GSC_SP[2500:], GSC_DP[2500:]), axis = 0)\n",
    "    Unseen_GSC = Unseen_GSC[:, 2:]\n",
    "    print(\"unseen_gsc\", Unseen_GSC.shape)\n",
    "\n",
    "    Sub_GSC_Dataset = np.concatenate((GSC_Sub_SP[:2500],GSC_Sub_DP[:2500]), axis = 0)\n",
    "    Sub_GSC_Dataset = Sub_GSC_Dataset[:, 2:]\n",
    "    print(\"Sub_gsc_dataset\", Sub_GSC_Dataset.shape)\n",
    "\n",
    "    Sub_Unseen_GSC = np.concatenate((GSC_Sub_SP[2500:], GSC_Sub_DP[2500:]), axis=0)\n",
    "    Sub_Unseen_GSC = Sub_Unseen_GSC[:, 2:]\n",
    "    print(\"Unseen_Sub_gsc\", Sub_Unseen_GSC.shape)\n",
    "\n",
    "\n",
    "    GSC_target = np.concatenate((GSC_SP_target[1:2501],GSC_DP_target[1:2501]), axis=0)\n",
    "    GSC_target = GSC_target.reshape(-1,1)\n",
    "    print(\"GSC_target\", GSC_target.shape)\n",
    "\n",
    "    Unseen_GSC_target = np.concatenate((GSC_SP_target[2500:4999],GSC_DP_target[2500:4999]), axis=0)\n",
    "    Unseen_GSC_target = Unseen_GSC_target.reshape(-1,1)\n",
    "    print(\"unseen_target\", Unseen_GSC_target.shape) \n",
    "\n",
    "    GSC_Final = np.concatenate((GSC_Dataset, GSC_target), axis=1)\n",
    "    print(\"GSC_final\", GSC_Final.shape)\n",
    "\n",
    "    GSC_Sub_Final = np.concatenate((Sub_GSC_Dataset, GSC_target), axis=1)\n",
    "    print(\"GSC_sub_final\", GSC_Sub_Final.shape )\n",
    "\n",
    "    #Generate unseen data\n",
    "    Unseen_GSC_Final = np.concatenate((Unseen_GSC, Unseen_GSC_target), axis=1)\n",
    "    print(\"Unseen_GSC_final\", Unseen_GSC_Final.shape)\n",
    "\n",
    "    Unseen_GSC_Sub_Final = np.concatenate((Sub_Unseen_GSC, Unseen_GSC_target), axis=1)\n",
    "    print(\"Unseen_GSC_sub_final\", Unseen_GSC_Sub_Final.shape)\n",
    "    \n",
    "    #Convert str to int\n",
    "    temp = GSC_Final.reshape(-1)\n",
    "    #temp.shape\n",
    "    List = temp.tolist()\n",
    "    List = [int(x) for x in List]\n",
    "    GSC_Final = np.array(List).reshape(5000,1025)\n",
    "    temp1 = GSC_Sub_Final.reshape(-1)\n",
    "    #temp1.shape\n",
    "    List = temp1.tolist()\n",
    "    List = [int(x) for x in List]\n",
    "    GSC_Sub_Final = np.array(List).reshape(5000,513)\n",
    "    temp2 = Unseen_GSC_Final.reshape(-1)\n",
    "    temp2.shape\n",
    "    List = temp2.tolist()\n",
    "    List = [int(x) for x in List]\n",
    "    Unseen_GSC_Final = np.array(List).reshape(4998,1025)\n",
    "    temp3 = Unseen_GSC_Sub_Final.reshape(-1)\n",
    "    temp3.shape\n",
    "    List = temp3.tolist()\n",
    "    List = [int(x) for x in List]\n",
    "    Unseen_GSC_Sub_Final = np.array(List).reshape(4998,513)\n",
    "\n",
    "    \n",
    "    return GSC_Final, GSC_Sub_Final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Select_GSC(GSC_Final, GSC_Sub_Final, Concatenated = True): #Select GSC dataset - Concatenation/Subtraction\n",
    "    if Concatenated == True:\n",
    "        print(\"GSC Dataset with feature concatenation\")\n",
    "        #GSC CONCATENATED\n",
    "        np.random.shuffle(GSC_Final)\n",
    "        #print(GSC_Final)\n",
    "        training_data = GSC_Final[:, :1024]\n",
    "        #print(training_data.shape)\n",
    "        target_data = GSC_Final[:, 1024:]\n",
    "        #print(target_data.shape)\n",
    "        #print(len(training_data[0]))\n",
    "        \n",
    "        RawData = np.transpose(training_data)\n",
    "        RawTarget = target_data\n",
    "        #print(RawData[500])\n",
    "\n",
    "        cols = []\n",
    "        var = []\n",
    "        for i in range(len(RawData)):\n",
    "            Var = np.var(RawData[i], axis=0)\n",
    "            var.append(Var)\n",
    "            if Var == 0 :\n",
    "                cols.append(i)\n",
    "        #print(cols)\n",
    "        #print(len(cols))\n",
    "        #print(var)\n",
    "\n",
    "        RawData = np.delete(RawData, cols, axis=0)\n",
    "        print(RawData.shape)\n",
    "        RawData = RawData + 0.00001*np.random.rand(RawData.shape[0], RawData.shape[1])\n",
    "        return RawData, RawTarget\n",
    "        \n",
    "    else:\n",
    "        print(\"GSC Dataset with feature subtraction\")\n",
    "        #GSC SUBTRACTED\n",
    "        np.random.shuffle(GSC_Sub_Final)\n",
    "        #print(GSC_Sub_Final)\n",
    "        training_data = GSC_Sub_Final[:, :512]\n",
    "        #print(training_data.shape)\n",
    "        target_data = GSC_Sub_Final[:, 512:]\n",
    "        #print(target_data.shape)\n",
    "        #print(len(training_data[0]))\n",
    "        \n",
    "        RawData = np.transpose(training_data)\n",
    "        RawTarget = target_data\n",
    "        #print(RawData[500])\n",
    "\n",
    "        cols = []\n",
    "        var = []\n",
    "        for i in range(len(RawData)):\n",
    "            Var = np.var(RawData[i], axis=0)\n",
    "            var.append(Var)\n",
    "            if Var == 0 :\n",
    "                cols.append(i)\n",
    "        #print(cols)\n",
    "        #print(len(cols))\n",
    "        #print(var)\n",
    "\n",
    "        RawData = np.delete(RawData, cols, axis=0)\n",
    "        print(RawData.shape)\n",
    "        RawData = RawData + 0.00001*np.random.rand(RawData.shape[0], RawData.shape[1])\n",
    "        return RawData, RawTarget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of gsc same pairs 5000\n",
      "GSC_SP (4999, 1026)\n",
      "GSC_SUB_SP (4999, 514)\n",
      "GSC_DP (4999, 1026)\n",
      "GSC_SUB_DP (4999, 514)\n",
      "GSC_Dataset (5000, 1024)\n",
      "unseen_gsc (4998, 1024)\n",
      "Sub_gsc_dataset (5000, 512)\n",
      "Unseen_Sub_gsc (4998, 512)\n",
      "GSC_target (5000, 1)\n",
      "unseen_target (4998, 1)\n",
      "GSC_final (5000, 1025)\n",
      "GSC_sub_final (5000, 513)\n",
      "Unseen_GSC_final (4998, 1025)\n",
      "Unseen_GSC_sub_final (4998, 513)\n"
     ]
    }
   ],
   "source": [
    "GSC_Final, GSC_Sub_Final = create_gsc_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GenerateTrainingDataMatrix(rawData, TrainingPercent = 80): #training data split(80%)\n",
    "    T_len = int(math.ceil(len(rawData[0])*0.01*TrainingPercent))\n",
    "    #print(T_len)\n",
    "    d2 = rawData[:,0:T_len]\n",
    "    #print(str(TrainingPercent) + \"% Training Data Generated..\")\n",
    "    return d2\n",
    "\n",
    "def GenerateTrainingTarget(rawTraining,TrainingPercent = 80): #training target split\n",
    "    TrainingLen = int(math.ceil(len(rawTraining)*(TrainingPercent*0.01)))\n",
    "    t           = rawTraining[:TrainingLen]\n",
    "    #print(TrainingLen)\n",
    "    #print(str(TrainingPercent) + \"% Training Target Generated..\")\n",
    "    return t\n",
    "\n",
    "def GenerateValData(rawData, ValPercent, TrainingCount): #Generate validation data set as with 10% split\n",
    "    valSize = int(math.ceil(len(rawData[0])*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    dataMatrix = rawData[:,TrainingCount+1:V_End]\n",
    "    #print (str(ValPercent) + \"% Val Data Generated..\")  \n",
    "    return dataMatrix\n",
    "\n",
    "def GenerateValTargetVector(rawData, ValPercent, TrainingCount): #Validation target split \n",
    "    valSize = int(math.ceil(len(rawData)*ValPercent*0.01))\n",
    "    V_End = TrainingCount + valSize\n",
    "    t =rawData[TrainingCount+1:V_End]\n",
    "    #print (str(ValPercent) + \"% Val Target Data Generated..\")\n",
    "    return t\n",
    "def GenerateBigSigma(Data, MuMatrix,TrainingPercent,IsSynthetic): #Generates a Diagonal matrix(41,41) of variances of all features\n",
    "    BigSigma    = np.zeros((len(Data),len(Data)))\n",
    "    #print (BigSigma)\n",
    "    DataT       = np.transpose(Data)\n",
    "    #print (DataT)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))        \n",
    "    varVect     = []\n",
    "    for i in range(0,len(DataT[0])):\n",
    "        vct = []\n",
    "        for j in range(0,int(TrainingLen)):\n",
    "            vct.append(Data[i][j])    \n",
    "        varVect.append(np.var(vct)) #calculating variance of data.\n",
    "        \n",
    "    \n",
    "    for j in range(len(Data)):\n",
    "        BigSigma[j][j] = varVect[j]\n",
    "    if IsSynthetic == True:\n",
    "        BigSigma = np.dot(3,BigSigma)\n",
    "    else:\n",
    "        BigSigma = np.dot(200,BigSigma)\n",
    "    ##print (\"BigSigma Generated..\")\n",
    "    #print(BigSigma)\n",
    "    return BigSigma\n",
    "\n",
    "def GetScalar(DataRow,MuRow, BigSigInv):  \n",
    "    R = np.subtract(DataRow,MuRow) #returns (M,41) matrix\n",
    "    T = np.dot(BigSigInv,np.transpose(R))  #returns (41,M) matrix\n",
    "    L = np.dot(R,T) #computes dot product of the two matrices\n",
    "    return L\n",
    "\n",
    "def GetRadialBasisOut(DataRow,MuRow, BigSigInv):    \n",
    "    phi_x = math.exp(-0.5*GetScalar(DataRow,MuRow,BigSigInv)) #(M,41).(41,41).(41,M) matrices as per the radial basis formula\n",
    "    return phi_x\n",
    "\n",
    "def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingPercent = 80):\n",
    "    DataT = np.transpose(Data)\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))         \n",
    "    PHI = np.zeros((int(TrainingLen),len(MuMatrix))) \n",
    "    BigSigInv = np.linalg.inv(BigSigma) #inverse the big sigma matrix\n",
    "    for  C in range(0,len(MuMatrix)):\n",
    "        for R in range(0,int(TrainingLen)):\n",
    "            PHI[R][C] = GetRadialBasisOut(DataT[R], MuMatrix[C], BigSigInv)\n",
    "    #print (\"PHI Generated..\")\n",
    "    return PHI #returns matrix of gaussian radial function.\n",
    "\n",
    "def GetWeightsClosedForm(PHI, T, Lambda): #Generates the weights as per the closed form formula\n",
    "    Lambda_I = np.identity(len(PHI[0])) #I matrix from the closed form formula\n",
    "    for i in range(0,len(PHI[0])): \n",
    "        Lambda_I[i][i] = Lambda #Lambda matrix from the formula\n",
    "    PHI_T       = np.transpose(PHI) #PHI from the forumla\n",
    "    PHI_SQR     = np.dot(PHI_T,PHI) # PHI . PHI transpose as seen in the formula\n",
    "    PHI_SQR_LI  = np.add(Lambda_I,PHI_SQR) # PHI. PHI transpose + Lambda*I \n",
    "    PHI_SQR_INV = np.linalg.inv(PHI_SQR_LI) # Inverse of PHI. PHI transpose + Lambda*I \n",
    "    INTER       = np.dot(PHI_SQR_INV, PHI_T) #Inverse of (PHI. PHI transpose + Lambda*I) . PHI Transpose\n",
    "    W           = np.dot(INTER, T) ##Inverse of (PHI. PHI transpose + Lambda*I) . PHI Transpose . T \"Closed form equation\"\n",
    "    ##print (\"Training Weights Generated..\")\n",
    "    return W\n",
    "\n",
    "def GetValTest(VAL_PHI,W): #Peforms prediction to get final output.\n",
    "    Y = np.dot(W,np.transpose(VAL_PHI))\n",
    "    ##print (\"Test Out Generated..\")\n",
    "    return Y\n",
    "\n",
    "def GetErms(VAL_TEST_OUT,ValDataAct): #Calculates RMS values of given data set.\n",
    "    sum = 0.0\n",
    "    t=0\n",
    "    accuracy = 0.0\n",
    "    counter = 0\n",
    "    val = 0.0\n",
    "    for i in range (0,len(VAL_TEST_OUT)):\n",
    "        sum = sum + math.pow((ValDataAct[i] - VAL_TEST_OUT[i]),2) #sum of squared errors.\n",
    "        if(int(np.around(VAL_TEST_OUT[i], 0)) == ValDataAct[i]):\n",
    "            counter+=1\n",
    "    accuracy = (float((counter*100))/float(len(VAL_TEST_OUT)))\n",
    "    ##print (\"Accuracy Generated..\")\n",
    "    ##print (\"Validation E_RMS : \" + str(math.sqrt(sum/len(VAL_TEST_OUT))))\n",
    "    return (str(accuracy) + ',' +  str(math.sqrt(sum/len(VAL_TEST_OUT))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Closed Form Solution [Finding Weights using Moore- Penrose pseudo- Inverse Matrix] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_Regression(TrainingData, TrainingTarget, ValData, ValDataAct, TestData, TestDataAct, Concatenate = True):\n",
    "    ErmsArr = []\n",
    "    AccuracyArr = []\n",
    "    X_kmeans = np.transpose(TrainingData)\n",
    "    kmeans = KMeans(n_clusters=M, random_state=0).fit(np.transpose(TrainingData))\n",
    "    #print (kmeans)\n",
    "    y_kmeans = kmeans.predict(X_kmeans)\n",
    "    #print (len(y_kmeans))\n",
    "    Mu = kmeans.cluster_centers_ #find the center of clusters from traning data\n",
    "    #print(Mu)\n",
    "    BigSigma     = GenerateBigSigma(RawData, Mu, TrainingPercent,IsSynthetic)\n",
    "    TRAINING_PHI = GetPhiMatrix(RawData, Mu, BigSigma, TrainingPercent)\n",
    "    W            = GetWeightsClosedForm(TRAINING_PHI,TrainingTarget,(C_Lambda)) \n",
    "    TEST_PHI     = GetPhiMatrix(TestData, Mu, BigSigma, 100) \n",
    "    VAL_PHI      = GetPhiMatrix(ValData, Mu, BigSigma, 100)\n",
    "\n",
    "    #print(Mu.shape)\n",
    "    #print(BigSigma.shape)\n",
    "    #print(TRAINING_PHI.shape)\n",
    "    #print(W.shape)\n",
    "    #print(VAL_PHI.shape)\n",
    "    #print(TEST_PHI.shape)\n",
    "\n",
    "    TR_TEST_OUT  = GetValTest(TRAINING_PHI,W)\n",
    "    VAL_TEST_OUT = GetValTest(VAL_PHI,W)\n",
    "    TEST_OUT     = GetValTest(TEST_PHI,W)\n",
    "    #print(TEST_OUT)\n",
    "    TestAcc = []\n",
    "    TrainingAccuracy   = str(GetErms(TR_TEST_OUT,TrainingTarget))\n",
    "    ValidationAccuracy = str(GetErms(VAL_TEST_OUT,ValDataAct))\n",
    "    TestAccuracy       = str(GetErms(TEST_OUT,TestDataAct))\n",
    "    #TestAcc.append[TestAccuracy]\n",
    "    #print (TestAccuracy)\n",
    "\n",
    "    W_Now        = np.dot(220, W)\n",
    "    La           = 0.5 #Regularization constant\n",
    "    learningRate = 0.2\n",
    "    L_Erms_Val   = []\n",
    "    L_Erms_TR    = []\n",
    "    L_Erms_Test  = []\n",
    "    W_Mat        = []\n",
    "\n",
    "    for i in range(0,1000):\n",
    "\n",
    "        #print ('---------Iteration: ' + str(i) + '--------------')\n",
    "        Delta_E_D     = -np.dot((TrainingTarget[i] - np.dot(np.transpose(W_Now),TRAINING_PHI[i])),TRAINING_PHI[i])\n",
    "        La_Delta_E_W  = np.dot(La,W_Now)\n",
    "        Delta_E       = np.add(Delta_E_D,La_Delta_E_W)    \n",
    "        Delta_W       = -np.dot(learningRate,Delta_E)\n",
    "        W_T_Next      = W_Now + Delta_W\n",
    "        W_Now         = W_T_Next\n",
    "\n",
    "        #-----------------TrainingData Accuracy---------------------#\n",
    "        TR_TEST_OUT   = GetValTest(TRAINING_PHI,W_T_Next) \n",
    "        Erms_TR       = GetErms(TR_TEST_OUT,TrainingTarget)\n",
    "        L_Erms_TR.append(float(Erms_TR.split(',')[1]))\n",
    "\n",
    "        #-----------------ValidationData Accuracy---------------------#\n",
    "        VAL_TEST_OUT  = GetValTest(VAL_PHI,W_T_Next) \n",
    "        Erms_Val      = GetErms(VAL_TEST_OUT,ValDataAct)\n",
    "        L_Erms_Val.append(float(Erms_Val.split(',')[1]))\n",
    "\n",
    "        #-----------------TestingData Accuracy---------------------#\n",
    "        GD_TEST_OUT      = GetValTest(TEST_PHI,W_T_Next) \n",
    "        Erms_Test = GetErms(GD_TEST_OUT,TestDataAct)\n",
    "        L_Erms_Test.append(float(Erms_Test.split(',')[1]))\n",
    "\n",
    "    print ('----------Gradient Descent Solution--------------------')\n",
    "    if (Concatenate == True):\n",
    "        print(\"E_rms for Human Observed Dataset with feature concatenation\")\n",
    "        #print (\"Lambda  = 0.0001\\neta=0.01\")\n",
    "        print (\"E_rms Training   = \" + str(np.around(min(L_Erms_TR),5)))\n",
    "        print (\"E_rms Validation = \" + str(np.around(min(L_Erms_Val),5)))\n",
    "        print (\"E_rms Testing    = \" + str(np.around(min(L_Erms_Test),5)))\n",
    "    else:\n",
    "        print(\"E_rms for Human Observed Dataset with feature subtraction\")\n",
    "        #print (\"Lambda  = 0.0001\\neta=0.01\")\n",
    "        print (\"E_rms Training   = \" + str(np.around(min(L_Erms_TR),5)))\n",
    "        print (\"E_rms Validation = \" + str(np.around(min(L_Erms_Val),5)))\n",
    "        print (\"E_rms Testing    = \" + str(np.around(min(L_Erms_Test),5)))\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Observed Dataset with Concatenation selected\n",
      "(1582, 18)\n",
      "----------Gradient Descent Solution--------------------\n",
      "E_rms for Human Observed Dataset with feature concatenation\n",
      "E_rms Training   = 0.35639\n",
      "E_rms Validation = 0.36046\n",
      "E_rms Testing    = 0.35066\n",
      "Human Observed Dataset with Subtraction selected\n",
      "----------Gradient Descent Solution--------------------\n",
      "E_rms for Human Observed Dataset with feature subtraction\n",
      "E_rms Training   = 0.45775\n",
      "E_rms Validation = 0.42991\n",
      "E_rms Testing    = 0.4342\n"
     ]
    }
   ],
   "source": [
    "#HUMAN OBSERVED DATASET\n",
    "RawData, RawTarget = Select_HODS(Final_HODS, Final_SHODS, Concatenated = True)\n",
    "#print(RawData.shape, RawTarget.shape)\n",
    "TrainingData, TrainingTarget, ValData, ValDataAct, TestData, TestDataAct = Generate_Data_Split(RawData, RawTarget)\n",
    "Linear_Regression(TrainingData, TrainingTarget, ValData, ValDataAct, TestData, TestDataAct, Concatenate = True)\n",
    "#FeatureSubtraced Dataset\n",
    "RawData, RawTarget = Select_HODS(Final_HODS, Final_SHODS, Concatenated = False)\n",
    "TrainingData, TrainingTarget, ValData, ValDataAct, TestData, TestDataAct = Generate_Data_Split(RawData, RawTarget)\n",
    "#print(RawData.shape, RawTarget.shape)\n",
    "Linear_Regression(TrainingData, TrainingTarget, ValData, ValDataAct, TestData, TestDataAct, Concatenate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSC Dataset with feature concatenation\n",
      "(940, 5000)\n",
      "(940, 5000) (5000, 1)\n",
      "----------Gradient Descent Solution--------------------\n",
      "E_rms for Human Observed Dataset with feature concatenation\n",
      "E_rms Training   = 0.38227\n",
      "E_rms Validation = 0.40908\n",
      "E_rms Testing    = 0.38213\n",
      "GSC Dataset with feature subtraction\n",
      "(474, 5000)\n",
      "----------Gradient Descent Solution--------------------\n",
      "E_rms for Human Observed Dataset with feature subtraction\n",
      "E_rms Training   = 0.41219\n",
      "E_rms Validation = 0.42542\n",
      "E_rms Testing    = 0.38851\n"
     ]
    }
   ],
   "source": [
    "#GSC DATASET\n",
    "RawData, RawTarget = Select_GSC(GSC_Final, GSC_Sub_Final, Concatenated = True)\n",
    "print(RawData.shape, RawTarget.shape)\n",
    "TrainingData, TrainingTarget, ValData, ValDataAct, TestData, TestDataAct = Generate_Data_Split(RawData, RawTarget)\n",
    "Linear_Regression(TrainingData, TrainingTarget, ValData, ValDataAct, TestData, TestDataAct, Concatenate = True)\n",
    "#FeatureSubtraced Dataset\n",
    "RawData, RawTarget = Select_GSC(GSC_Final, GSC_Sub_Final, Concatenated = False)\n",
    "TrainingData, TrainingTarget, ValData, ValDataAct, TestData, TestDataAct = Generate_Data_Split(RawData, RawTarget)\n",
    "#print(RawData.shape, RawTarget.shape)\n",
    "Linear_Regression(TrainingData, TrainingTarget, ValData, ValDataAct, TestData, TestDataAct, Concatenate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "drop_out = 0.2\n",
    "first_dense_layer_nodes  = 256\n",
    "hidden_dense_layer = 512\n",
    "second_dense_layer_nodes = 1\n",
    "\n",
    "def get_model(input_size):\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(first_dense_layer_nodes, input_dim=input_size))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dense(hidden_dense_layer))\n",
    "    model.add(Activation('relu')) \n",
    "    \n",
    "    # Why dropout?\n",
    "    model.add(Dropout(drop_out))\n",
    "    \n",
    "    model.add(Dense(second_dense_layer_nodes))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Neural_Net(TrainingData, TrainingTarget):\n",
    "    validation_data_split = 0.2\n",
    "    num_epochs = 1000\n",
    "    model_batch_size = 128\n",
    "    tb_batch_size = 32\n",
    "    early_patience = 100\n",
    "\n",
    "    tensorboard_cb   = TensorBoard(log_dir='logs', batch_size= tb_batch_size, write_graph= True)\n",
    "    earlystopping_cb = EarlyStopping(monitor='val_loss', verbose=1, patience=early_patience, mode='min')\n",
    "\n",
    "    history = model.fit(TrainingData.T\n",
    "                        , TrainingTarget\n",
    "                        , validation_split=validation_data_split\n",
    "                        , epochs=num_epochs\n",
    "                        , batch_size=model_batch_size\n",
    "                        , callbacks = [tensorboard_cb,earlystopping_cb]\n",
    "                       )\n",
    "    scores = model.evaluate(TrainingData.T, TrainingTarget)\n",
    "    print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_Graph(history):\n",
    "    %matplotlib inline\n",
    "    df = pd.DataFrame(history.history)\n",
    "    df.plot(subplots=True, grid=True, figsize=(10,15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_Accuracy(TestData, TestDataAct):\n",
    "        wrong   = 0\n",
    "        right   = 0\n",
    "\n",
    "        for i,j in zip(TestData.T, TestDataAct):\n",
    "            y = model.predict(np.array(i).reshape(-1,TestData.shape[0]))\n",
    "            #predictedTestLabel.append(decodeLabel(y.argmax()))\n",
    "\n",
    "            if j.argmax() == y.argmax():\n",
    "                right = right + 1\n",
    "            else:\n",
    "                wrong = wrong + 1\n",
    "\n",
    "        print(\"Errors: \" + str(wrong), \" Correct :\" + str(right))\n",
    "\n",
    "        print(\"Testing Accuracy: \" + str(right/(right+wrong)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Observed Dataset with Concatenation selected\n",
      "(1582, 18)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 256)               4864      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 513       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 136,961\n",
      "Trainable params: 136,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1012 samples, validate on 254 samples\n",
      "Epoch 1/1000\n",
      "1012/1012 [==============================] - 1s 593us/step - loss: 0.5486 - acc: 0.7717 - val_loss: 0.4266 - val_acc: 0.8268\n",
      "Epoch 2/1000\n",
      "1012/1012 [==============================] - 0s 80us/step - loss: 0.3159 - acc: 0.8913 - val_loss: 0.3247 - val_acc: 0.8504\n",
      "Epoch 3/1000\n",
      "1012/1012 [==============================] - 0s 83us/step - loss: 0.2260 - acc: 0.9269 - val_loss: 0.2962 - val_acc: 0.8780\n",
      "Epoch 4/1000\n",
      "1012/1012 [==============================] - 0s 88us/step - loss: 0.1806 - acc: 0.9407 - val_loss: 0.2143 - val_acc: 0.9331\n",
      "Epoch 5/1000\n",
      "1012/1012 [==============================] - 0s 86us/step - loss: 0.1466 - acc: 0.9585 - val_loss: 0.1905 - val_acc: 0.9449\n",
      "Epoch 6/1000\n",
      "1012/1012 [==============================] - 0s 100us/step - loss: 0.1236 - acc: 0.9615 - val_loss: 0.1611 - val_acc: 0.9488\n",
      "Epoch 7/1000\n",
      "1012/1012 [==============================] - 0s 81us/step - loss: 0.1181 - acc: 0.9634 - val_loss: 0.1621 - val_acc: 0.9370\n",
      "Epoch 8/1000\n",
      "1012/1012 [==============================] - 0s 91us/step - loss: 0.1067 - acc: 0.9694 - val_loss: 0.1479 - val_acc: 0.9528\n",
      "Epoch 9/1000\n",
      "1012/1012 [==============================] - 0s 92us/step - loss: 0.0893 - acc: 0.9723 - val_loss: 0.1350 - val_acc: 0.9567\n",
      "Epoch 10/1000\n",
      "1012/1012 [==============================] - 0s 100us/step - loss: 0.0818 - acc: 0.9733 - val_loss: 0.1409 - val_acc: 0.9528\n",
      "Epoch 11/1000\n",
      "1012/1012 [==============================] - 0s 103us/step - loss: 0.0831 - acc: 0.9792 - val_loss: 0.1283 - val_acc: 0.9567\n",
      "Epoch 12/1000\n",
      "1012/1012 [==============================] - 0s 99us/step - loss: 0.0715 - acc: 0.9802 - val_loss: 0.1112 - val_acc: 0.9646\n",
      "Epoch 13/1000\n",
      "1012/1012 [==============================] - 0s 85us/step - loss: 0.0680 - acc: 0.9812 - val_loss: 0.1123 - val_acc: 0.9646\n",
      "Epoch 14/1000\n",
      "1012/1012 [==============================] - 0s 91us/step - loss: 0.0662 - acc: 0.9812 - val_loss: 0.1458 - val_acc: 0.9567\n",
      "Epoch 15/1000\n",
      "1012/1012 [==============================] - 0s 79us/step - loss: 0.0653 - acc: 0.9822 - val_loss: 0.1724 - val_acc: 0.9449\n",
      "Epoch 16/1000\n",
      "1012/1012 [==============================] - 0s 80us/step - loss: 0.0725 - acc: 0.9822 - val_loss: 0.0964 - val_acc: 0.9685\n",
      "Epoch 17/1000\n",
      "1012/1012 [==============================] - 0s 84us/step - loss: 0.0583 - acc: 0.9852 - val_loss: 0.0915 - val_acc: 0.9724\n",
      "Epoch 18/1000\n",
      "1012/1012 [==============================] - 0s 78us/step - loss: 0.0498 - acc: 0.9852 - val_loss: 0.0875 - val_acc: 0.9764\n",
      "Epoch 19/1000\n",
      "1012/1012 [==============================] - 0s 77us/step - loss: 0.0480 - acc: 0.9881 - val_loss: 0.0829 - val_acc: 0.9764\n",
      "Epoch 20/1000\n",
      "1012/1012 [==============================] - 0s 88us/step - loss: 0.0479 - acc: 0.9862 - val_loss: 0.1206 - val_acc: 0.9646\n",
      "Epoch 21/1000\n",
      "1012/1012 [==============================] - 0s 83us/step - loss: 0.0483 - acc: 0.9881 - val_loss: 0.1120 - val_acc: 0.9685\n",
      "Epoch 22/1000\n",
      "1012/1012 [==============================] - 0s 98us/step - loss: 0.0451 - acc: 0.9852 - val_loss: 0.0855 - val_acc: 0.9843\n",
      "Epoch 23/1000\n",
      "1012/1012 [==============================] - 0s 102us/step - loss: 0.0446 - acc: 0.9872 - val_loss: 0.0753 - val_acc: 0.9803\n",
      "Epoch 24/1000\n",
      "1012/1012 [==============================] - 0s 83us/step - loss: 0.0348 - acc: 0.9941 - val_loss: 0.1069 - val_acc: 0.9685\n",
      "Epoch 25/1000\n",
      "1012/1012 [==============================] - 0s 92us/step - loss: 0.0388 - acc: 0.9881 - val_loss: 0.0777 - val_acc: 0.9764\n",
      "Epoch 26/1000\n",
      "1012/1012 [==============================] - 0s 80us/step - loss: 0.0318 - acc: 0.9901 - val_loss: 0.0713 - val_acc: 0.9764\n",
      "Epoch 27/1000\n",
      "1012/1012 [==============================] - 0s 81us/step - loss: 0.0271 - acc: 0.9941 - val_loss: 0.0751 - val_acc: 0.9764\n",
      "Epoch 28/1000\n",
      "1012/1012 [==============================] - 0s 82us/step - loss: 0.0278 - acc: 0.9911 - val_loss: 0.0752 - val_acc: 0.9764\n",
      "Epoch 29/1000\n",
      "1012/1012 [==============================] - 0s 89us/step - loss: 0.0261 - acc: 0.9921 - val_loss: 0.0763 - val_acc: 0.9764\n",
      "Epoch 30/1000\n",
      "1012/1012 [==============================] - 0s 82us/step - loss: 0.0283 - acc: 0.9911 - val_loss: 0.0919 - val_acc: 0.9764\n",
      "Epoch 31/1000\n",
      "1012/1012 [==============================] - 0s 83us/step - loss: 0.0288 - acc: 0.9911 - val_loss: 0.0639 - val_acc: 0.9803\n",
      "Epoch 32/1000\n",
      "1012/1012 [==============================] - 0s 88us/step - loss: 0.0301 - acc: 0.9901 - val_loss: 0.0688 - val_acc: 0.9921\n",
      "Epoch 33/1000\n",
      "1012/1012 [==============================] - 0s 84us/step - loss: 0.0325 - acc: 0.9941 - val_loss: 0.1014 - val_acc: 0.9764\n",
      "Epoch 34/1000\n",
      "1012/1012 [==============================] - 0s 81us/step - loss: 0.0264 - acc: 0.9901 - val_loss: 0.0775 - val_acc: 0.9764\n",
      "Epoch 35/1000\n",
      "1012/1012 [==============================] - 0s 71us/step - loss: 0.0218 - acc: 0.9921 - val_loss: 0.0636 - val_acc: 0.9803\n",
      "Epoch 36/1000\n",
      "1012/1012 [==============================] - 0s 72us/step - loss: 0.0207 - acc: 0.9931 - val_loss: 0.0618 - val_acc: 0.9882\n",
      "Epoch 37/1000\n",
      "1012/1012 [==============================] - 0s 85us/step - loss: 0.0181 - acc: 0.9970 - val_loss: 0.0709 - val_acc: 0.9803\n",
      "Epoch 38/1000\n",
      "1012/1012 [==============================] - 0s 90us/step - loss: 0.0172 - acc: 0.9970 - val_loss: 0.0841 - val_acc: 0.9764\n",
      "Epoch 39/1000\n",
      "1012/1012 [==============================] - 0s 94us/step - loss: 0.0193 - acc: 0.9951 - val_loss: 0.0608 - val_acc: 0.9882\n",
      "Epoch 40/1000\n",
      "1012/1012 [==============================] - 0s 91us/step - loss: 0.0147 - acc: 0.9951 - val_loss: 0.0650 - val_acc: 0.9803\n",
      "Epoch 41/1000\n",
      "1012/1012 [==============================] - 0s 91us/step - loss: 0.0142 - acc: 0.9960 - val_loss: 0.0620 - val_acc: 0.9843\n",
      "Epoch 42/1000\n",
      "1012/1012 [==============================] - 0s 79us/step - loss: 0.0122 - acc: 0.9960 - val_loss: 0.0624 - val_acc: 0.9843\n",
      "Epoch 43/1000\n",
      "1012/1012 [==============================] - 0s 88us/step - loss: 0.0120 - acc: 0.9980 - val_loss: 0.0605 - val_acc: 0.9843\n",
      "Epoch 44/1000\n",
      "1012/1012 [==============================] - 0s 91us/step - loss: 0.0114 - acc: 0.9970 - val_loss: 0.0657 - val_acc: 0.9843\n",
      "Epoch 45/1000\n",
      "1012/1012 [==============================] - 0s 75us/step - loss: 0.0117 - acc: 0.9960 - val_loss: 0.0658 - val_acc: 0.9803\n",
      "Epoch 46/1000\n",
      "1012/1012 [==============================] - 0s 91us/step - loss: 0.0125 - acc: 0.9960 - val_loss: 0.0605 - val_acc: 0.9843\n",
      "Epoch 47/1000\n",
      "1012/1012 [==============================] - 0s 89us/step - loss: 0.0103 - acc: 0.9980 - val_loss: 0.0605 - val_acc: 0.9843\n",
      "Epoch 48/1000\n",
      "1012/1012 [==============================] - 0s 87us/step - loss: 0.0112 - acc: 0.9980 - val_loss: 0.0593 - val_acc: 0.9882\n",
      "Epoch 49/1000\n",
      "1012/1012 [==============================] - 0s 88us/step - loss: 0.0102 - acc: 0.9970 - val_loss: 0.0617 - val_acc: 0.9843\n",
      "Epoch 50/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012/1012 [==============================] - 0s 91us/step - loss: 0.0086 - acc: 0.9990 - val_loss: 0.0675 - val_acc: 0.9803\n",
      "Epoch 51/1000\n",
      "1012/1012 [==============================] - 0s 91us/step - loss: 0.0085 - acc: 0.9980 - val_loss: 0.0625 - val_acc: 0.9843\n",
      "Epoch 52/1000\n",
      "1012/1012 [==============================] - 0s 94us/step - loss: 0.0084 - acc: 0.9990 - val_loss: 0.0595 - val_acc: 0.9882\n",
      "Epoch 53/1000\n",
      "1012/1012 [==============================] - 0s 96us/step - loss: 0.0078 - acc: 0.9980 - val_loss: 0.0618 - val_acc: 0.9843\n",
      "Epoch 54/1000\n",
      "1012/1012 [==============================] - 0s 90us/step - loss: 0.0074 - acc: 0.9980 - val_loss: 0.0730 - val_acc: 0.9803\n",
      "Epoch 55/1000\n",
      "1012/1012 [==============================] - 0s 96us/step - loss: 0.0081 - acc: 0.9980 - val_loss: 0.0643 - val_acc: 0.9843\n",
      "Epoch 56/1000\n",
      "1012/1012 [==============================] - 0s 90us/step - loss: 0.0078 - acc: 0.9980 - val_loss: 0.0585 - val_acc: 0.9921\n",
      "Epoch 57/1000\n",
      "1012/1012 [==============================] - 0s 91us/step - loss: 0.0058 - acc: 0.9990 - val_loss: 0.0721 - val_acc: 0.9803\n",
      "Epoch 58/1000\n",
      "1012/1012 [==============================] - 0s 93us/step - loss: 0.0068 - acc: 0.9980 - val_loss: 0.0585 - val_acc: 0.9843\n",
      "Epoch 59/1000\n",
      "1012/1012 [==============================] - 0s 89us/step - loss: 0.0082 - acc: 0.9980 - val_loss: 0.0642 - val_acc: 0.9843\n",
      "Epoch 60/1000\n",
      "1012/1012 [==============================] - 0s 92us/step - loss: 0.0065 - acc: 0.9990 - val_loss: 0.0757 - val_acc: 0.9803\n",
      "Epoch 61/1000\n",
      "1012/1012 [==============================] - 0s 95us/step - loss: 0.0070 - acc: 0.9970 - val_loss: 0.0605 - val_acc: 0.9882\n",
      "Epoch 62/1000\n",
      "1012/1012 [==============================] - 0s 89us/step - loss: 0.0053 - acc: 0.9990 - val_loss: 0.0604 - val_acc: 0.9882\n",
      "Epoch 63/1000\n",
      "1012/1012 [==============================] - 0s 98us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.0590 - val_acc: 0.9921\n",
      "Epoch 64/1000\n",
      "1012/1012 [==============================] - 0s 91us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 0.0618 - val_acc: 0.9843\n",
      "Epoch 65/1000\n",
      "1012/1012 [==============================] - 0s 94us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 0.0639 - val_acc: 0.9843\n",
      "Epoch 66/1000\n",
      "1012/1012 [==============================] - 0s 88us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 0.0734 - val_acc: 0.9803\n",
      "Epoch 67/1000\n",
      "1012/1012 [==============================] - 0s 92us/step - loss: 0.0047 - acc: 0.9990 - val_loss: 0.0588 - val_acc: 0.9921\n",
      "Epoch 68/1000\n",
      "1012/1012 [==============================] - 0s 93us/step - loss: 0.0047 - acc: 0.9980 - val_loss: 0.0621 - val_acc: 0.9882\n",
      "Epoch 69/1000\n",
      "1012/1012 [==============================] - 0s 88us/step - loss: 0.0051 - acc: 0.9980 - val_loss: 0.0627 - val_acc: 0.9882\n",
      "Epoch 70/1000\n",
      "1012/1012 [==============================] - 0s 91us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 0.0624 - val_acc: 0.9843\n",
      "Epoch 71/1000\n",
      "1012/1012 [==============================] - 0s 86us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0697 - val_acc: 0.9803\n",
      "Epoch 72/1000\n",
      "1012/1012 [==============================] - 0s 93us/step - loss: 0.0040 - acc: 0.9990 - val_loss: 0.0733 - val_acc: 0.9803\n",
      "Epoch 73/1000\n",
      "1012/1012 [==============================] - 0s 93us/step - loss: 0.0032 - acc: 0.9990 - val_loss: 0.0576 - val_acc: 0.9921\n",
      "Epoch 74/1000\n",
      "1012/1012 [==============================] - 0s 89us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0698 - val_acc: 0.9803\n",
      "Epoch 75/1000\n",
      "1012/1012 [==============================] - 0s 89us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 0.0665 - val_acc: 0.9803\n",
      "Epoch 76/1000\n",
      "1012/1012 [==============================] - 0s 90us/step - loss: 0.0034 - acc: 0.9980 - val_loss: 0.0560 - val_acc: 0.9921\n",
      "Epoch 77/1000\n",
      "1012/1012 [==============================] - 0s 89us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.0905 - val_acc: 0.9803\n",
      "Epoch 78/1000\n",
      "1012/1012 [==============================] - 0s 90us/step - loss: 0.0041 - acc: 0.9990 - val_loss: 0.0533 - val_acc: 0.9921\n",
      "Epoch 79/1000\n",
      "1012/1012 [==============================] - 0s 88us/step - loss: 0.0063 - acc: 0.9980 - val_loss: 0.0636 - val_acc: 0.9843\n",
      "Epoch 80/1000\n",
      "1012/1012 [==============================] - 0s 93us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 0.0994 - val_acc: 0.9803\n",
      "Epoch 81/1000\n",
      "1012/1012 [==============================] - 0s 98us/step - loss: 0.0058 - acc: 0.9970 - val_loss: 0.0567 - val_acc: 0.9882\n",
      "Epoch 82/1000\n",
      "1012/1012 [==============================] - 0s 93us/step - loss: 0.0046 - acc: 0.9990 - val_loss: 0.0693 - val_acc: 0.9843\n",
      "Epoch 83/1000\n",
      "1012/1012 [==============================] - 0s 90us/step - loss: 0.0030 - acc: 0.9990 - val_loss: 0.0752 - val_acc: 0.9803\n",
      "Epoch 84/1000\n",
      "1012/1012 [==============================] - 0s 85us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0767 - val_acc: 0.9803\n",
      "Epoch 85/1000\n",
      "1012/1012 [==============================] - 0s 76us/step - loss: 0.0023 - acc: 0.9990 - val_loss: 0.0559 - val_acc: 0.9921\n",
      "Epoch 86/1000\n",
      "1012/1012 [==============================] - 0s 82us/step - loss: 0.0033 - acc: 0.9990 - val_loss: 0.0967 - val_acc: 0.9803\n",
      "Epoch 87/1000\n",
      "1012/1012 [==============================] - 0s 91us/step - loss: 0.0047 - acc: 0.9990 - val_loss: 0.0559 - val_acc: 0.9921\n",
      "Epoch 88/1000\n",
      "1012/1012 [==============================] - 0s 96us/step - loss: 0.0077 - acc: 0.9970 - val_loss: 0.0792 - val_acc: 0.9803\n",
      "Epoch 89/1000\n",
      "1012/1012 [==============================] - 0s 87us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 0.0742 - val_acc: 0.9803\n",
      "Epoch 90/1000\n",
      "1012/1012 [==============================] - 0s 89us/step - loss: 0.0040 - acc: 0.9980 - val_loss: 0.0598 - val_acc: 0.9921\n",
      "Epoch 91/1000\n",
      "1012/1012 [==============================] - 0s 88us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.0942 - val_acc: 0.9803\n",
      "Epoch 92/1000\n",
      "1012/1012 [==============================] - 0s 94us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 0.0569 - val_acc: 0.9921\n",
      "Epoch 93/1000\n",
      "1012/1012 [==============================] - 0s 80us/step - loss: 0.0020 - acc: 1.0000 - val_loss: 0.0783 - val_acc: 0.9803\n",
      "Epoch 94/1000\n",
      "1012/1012 [==============================] - 0s 74us/step - loss: 0.0021 - acc: 0.9990 - val_loss: 0.0562 - val_acc: 0.9921\n",
      "Epoch 95/1000\n",
      "1012/1012 [==============================] - 0s 75us/step - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0975 - val_acc: 0.9803\n",
      "Epoch 96/1000\n",
      "1012/1012 [==============================] - 0s 86us/step - loss: 0.0033 - acc: 0.9990 - val_loss: 0.0521 - val_acc: 0.9921\n",
      "Epoch 97/1000\n",
      "1012/1012 [==============================] - 0s 97us/step - loss: 0.0026 - acc: 1.0000 - val_loss: 0.1173 - val_acc: 0.9803\n",
      "Epoch 98/1000\n",
      "1012/1012 [==============================] - 0s 92us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0537 - val_acc: 0.9921\n",
      "Epoch 99/1000\n",
      "1012/1012 [==============================] - 0s 91us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 0.0895 - val_acc: 0.9803\n",
      "Epoch 100/1000\n",
      "1012/1012 [==============================] - 0s 95us/step - loss: 0.0037 - acc: 0.9970 - val_loss: 0.0566 - val_acc: 0.9921\n",
      "Epoch 101/1000\n",
      "1012/1012 [==============================] - 0s 89us/step - loss: 0.0081 - acc: 0.9980 - val_loss: 0.0905 - val_acc: 0.9803\n",
      "Epoch 102/1000\n",
      "1012/1012 [==============================] - 0s 92us/step - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0597 - val_acc: 0.9921\n",
      "Epoch 103/1000\n",
      "1012/1012 [==============================] - 0s 90us/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0955 - val_acc: 0.9803\n",
      "Epoch 104/1000\n",
      "1012/1012 [==============================] - 0s 73us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0683 - val_acc: 0.9882\n",
      "Epoch 105/1000\n",
      "1012/1012 [==============================] - 0s 73us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0733 - val_acc: 0.9803\n",
      "Epoch 106/1000\n",
      "1012/1012 [==============================] - 0s 82us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0670 - val_acc: 0.9882\n",
      "Epoch 107/1000\n",
      "1012/1012 [==============================] - 0s 87us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0763 - val_acc: 0.9803\n",
      "Epoch 108/1000\n",
      "1012/1012 [==============================] - 0s 84us/step - loss: 9.4661e-04 - acc: 1.0000 - val_loss: 0.0704 - val_acc: 0.9882\n",
      "Epoch 109/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012/1012 [==============================] - 0s 86us/step - loss: 8.2286e-04 - acc: 1.0000 - val_loss: 0.0867 - val_acc: 0.9803\n",
      "Epoch 110/1000\n",
      "1012/1012 [==============================] - 0s 79us/step - loss: 8.6736e-04 - acc: 1.0000 - val_loss: 0.0738 - val_acc: 0.9803\n",
      "Epoch 111/1000\n",
      "1012/1012 [==============================] - 0s 77us/step - loss: 6.7989e-04 - acc: 1.0000 - val_loss: 0.0682 - val_acc: 0.9921\n",
      "Epoch 112/1000\n",
      "1012/1012 [==============================] - 0s 86us/step - loss: 6.0903e-04 - acc: 1.0000 - val_loss: 0.0783 - val_acc: 0.9803\n",
      "Epoch 113/1000\n",
      "1012/1012 [==============================] - 0s 89us/step - loss: 6.7101e-04 - acc: 1.0000 - val_loss: 0.0818 - val_acc: 0.9803\n",
      "Epoch 114/1000\n",
      "1012/1012 [==============================] - 0s 80us/step - loss: 6.7765e-04 - acc: 1.0000 - val_loss: 0.0739 - val_acc: 0.9803\n",
      "Epoch 115/1000\n",
      "1012/1012 [==============================] - 0s 75us/step - loss: 5.5530e-04 - acc: 1.0000 - val_loss: 0.0830 - val_acc: 0.9803\n",
      "Epoch 116/1000\n",
      "1012/1012 [==============================] - 0s 88us/step - loss: 9.6108e-04 - acc: 1.0000 - val_loss: 0.0674 - val_acc: 0.9921\n",
      "Epoch 117/1000\n",
      "1012/1012 [==============================] - 0s 101us/step - loss: 7.1208e-04 - acc: 1.0000 - val_loss: 0.0771 - val_acc: 0.9803\n",
      "Epoch 118/1000\n",
      "1012/1012 [==============================] - 0s 91us/step - loss: 8.7974e-04 - acc: 1.0000 - val_loss: 0.0795 - val_acc: 0.9803\n",
      "Epoch 119/1000\n",
      "1012/1012 [==============================] - 0s 90us/step - loss: 8.4911e-04 - acc: 1.0000 - val_loss: 0.0808 - val_acc: 0.9803\n",
      "Epoch 120/1000\n",
      "1012/1012 [==============================] - 0s 91us/step - loss: 8.5224e-04 - acc: 1.0000 - val_loss: 0.0693 - val_acc: 0.9882\n",
      "Epoch 121/1000\n",
      "1012/1012 [==============================] - 0s 92us/step - loss: 6.9154e-04 - acc: 1.0000 - val_loss: 0.0851 - val_acc: 0.9803\n",
      "Epoch 122/1000\n",
      "1012/1012 [==============================] - 0s 93us/step - loss: 8.3570e-04 - acc: 1.0000 - val_loss: 0.0709 - val_acc: 0.9843\n",
      "Epoch 123/1000\n",
      "1012/1012 [==============================] - 0s 100us/step - loss: 6.2065e-04 - acc: 1.0000 - val_loss: 0.0639 - val_acc: 0.9921\n",
      "Epoch 124/1000\n",
      "1012/1012 [==============================] - 0s 88us/step - loss: 7.0142e-04 - acc: 1.0000 - val_loss: 0.0954 - val_acc: 0.9803\n",
      "Epoch 125/1000\n",
      "1012/1012 [==============================] - 0s 92us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0612 - val_acc: 0.9921\n",
      "Epoch 126/1000\n",
      "1012/1012 [==============================] - 0s 87us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0906 - val_acc: 0.9803\n",
      "Epoch 127/1000\n",
      "1012/1012 [==============================] - 0s 89us/step - loss: 5.7108e-04 - acc: 1.0000 - val_loss: 0.0745 - val_acc: 0.9843\n",
      "Epoch 128/1000\n",
      "1012/1012 [==============================] - 0s 94us/step - loss: 6.6601e-04 - acc: 1.0000 - val_loss: 0.0774 - val_acc: 0.9843\n",
      "Epoch 129/1000\n",
      "1012/1012 [==============================] - 0s 93us/step - loss: 7.5683e-04 - acc: 1.0000 - val_loss: 0.0783 - val_acc: 0.9843\n",
      "Epoch 130/1000\n",
      "1012/1012 [==============================] - 0s 90us/step - loss: 4.3703e-04 - acc: 1.0000 - val_loss: 0.0676 - val_acc: 0.9921\n",
      "Epoch 131/1000\n",
      "1012/1012 [==============================] - 0s 93us/step - loss: 9.9332e-04 - acc: 1.0000 - val_loss: 0.0980 - val_acc: 0.9803\n",
      "Epoch 132/1000\n",
      "1012/1012 [==============================] - 0s 81us/step - loss: 0.0015 - acc: 0.9990 - val_loss: 0.0625 - val_acc: 0.9921\n",
      "Epoch 133/1000\n",
      "1012/1012 [==============================] - 0s 87us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0898 - val_acc: 0.9803\n",
      "Epoch 134/1000\n",
      "1012/1012 [==============================] - 0s 88us/step - loss: 0.0017 - acc: 0.9990 - val_loss: 0.0641 - val_acc: 0.9921\n",
      "Epoch 135/1000\n",
      "1012/1012 [==============================] - 0s 88us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0863 - val_acc: 0.9803\n",
      "Epoch 136/1000\n",
      "1012/1012 [==============================] - 0s 89us/step - loss: 6.8825e-04 - acc: 1.0000 - val_loss: 0.0862 - val_acc: 0.9803\n",
      "Epoch 137/1000\n",
      "1012/1012 [==============================] - 0s 92us/step - loss: 4.4845e-04 - acc: 1.0000 - val_loss: 0.0729 - val_acc: 0.9843\n",
      "Epoch 138/1000\n",
      "1012/1012 [==============================] - 0s 89us/step - loss: 7.0736e-04 - acc: 1.0000 - val_loss: 0.0855 - val_acc: 0.9803\n",
      "Epoch 139/1000\n",
      "1012/1012 [==============================] - 0s 93us/step - loss: 5.1377e-04 - acc: 1.0000 - val_loss: 0.0930 - val_acc: 0.9803\n",
      "Epoch 140/1000\n",
      "1012/1012 [==============================] - 0s 90us/step - loss: 7.3876e-04 - acc: 1.0000 - val_loss: 0.0821 - val_acc: 0.9803\n",
      "Epoch 141/1000\n",
      "1012/1012 [==============================] - 0s 96us/step - loss: 3.7560e-04 - acc: 1.0000 - val_loss: 0.0858 - val_acc: 0.9803\n",
      "Epoch 142/1000\n",
      "1012/1012 [==============================] - 0s 100us/step - loss: 5.8098e-04 - acc: 1.0000 - val_loss: 0.0704 - val_acc: 0.9843\n",
      "Epoch 143/1000\n",
      "1012/1012 [==============================] - 0s 93us/step - loss: 4.7600e-04 - acc: 1.0000 - val_loss: 0.0799 - val_acc: 0.9803\n",
      "Epoch 144/1000\n",
      "1012/1012 [==============================] - 0s 90us/step - loss: 4.8060e-04 - acc: 1.0000 - val_loss: 0.0931 - val_acc: 0.9803\n",
      "Epoch 145/1000\n",
      "1012/1012 [==============================] - 0s 92us/step - loss: 3.4844e-04 - acc: 1.0000 - val_loss: 0.0794 - val_acc: 0.9803\n",
      "Epoch 146/1000\n",
      "1012/1012 [==============================] - 0s 88us/step - loss: 3.2942e-04 - acc: 1.0000 - val_loss: 0.0822 - val_acc: 0.9803\n",
      "Epoch 147/1000\n",
      "1012/1012 [==============================] - 0s 89us/step - loss: 3.5888e-04 - acc: 1.0000 - val_loss: 0.0866 - val_acc: 0.9803\n",
      "Epoch 148/1000\n",
      "1012/1012 [==============================] - 0s 95us/step - loss: 3.4123e-04 - acc: 1.0000 - val_loss: 0.0746 - val_acc: 0.9843\n",
      "Epoch 149/1000\n",
      "1012/1012 [==============================] - 0s 100us/step - loss: 2.9543e-04 - acc: 1.0000 - val_loss: 0.0846 - val_acc: 0.9803\n",
      "Epoch 150/1000\n",
      "1012/1012 [==============================] - 0s 81us/step - loss: 3.4324e-04 - acc: 1.0000 - val_loss: 0.0907 - val_acc: 0.9803\n",
      "Epoch 151/1000\n",
      "1012/1012 [==============================] - 0s 82us/step - loss: 4.6122e-04 - acc: 1.0000 - val_loss: 0.0812 - val_acc: 0.9803\n",
      "Epoch 152/1000\n",
      "1012/1012 [==============================] - 0s 90us/step - loss: 3.9811e-04 - acc: 1.0000 - val_loss: 0.0804 - val_acc: 0.9843\n",
      "Epoch 153/1000\n",
      "1012/1012 [==============================] - 0s 83us/step - loss: 2.8673e-04 - acc: 1.0000 - val_loss: 0.0763 - val_acc: 0.9843\n",
      "Epoch 154/1000\n",
      "1012/1012 [==============================] - 0s 85us/step - loss: 3.4753e-04 - acc: 1.0000 - val_loss: 0.0828 - val_acc: 0.9843\n",
      "Epoch 155/1000\n",
      "1012/1012 [==============================] - 0s 85us/step - loss: 2.8720e-04 - acc: 1.0000 - val_loss: 0.0803 - val_acc: 0.9843\n",
      "Epoch 156/1000\n",
      "1012/1012 [==============================] - 0s 85us/step - loss: 2.7018e-04 - acc: 1.0000 - val_loss: 0.0815 - val_acc: 0.9803\n",
      "Epoch 157/1000\n",
      "1012/1012 [==============================] - 0s 79us/step - loss: 2.4249e-04 - acc: 1.0000 - val_loss: 0.0834 - val_acc: 0.9803\n",
      "Epoch 158/1000\n",
      "1012/1012 [==============================] - 0s 96us/step - loss: 3.1816e-04 - acc: 1.0000 - val_loss: 0.0815 - val_acc: 0.9803\n",
      "Epoch 159/1000\n",
      "1012/1012 [==============================] - 0s 83us/step - loss: 3.0441e-04 - acc: 1.0000 - val_loss: 0.0799 - val_acc: 0.9843\n",
      "Epoch 160/1000\n",
      "1012/1012 [==============================] - 0s 86us/step - loss: 2.5710e-04 - acc: 1.0000 - val_loss: 0.0851 - val_acc: 0.9803\n",
      "Epoch 161/1000\n",
      "1012/1012 [==============================] - 0s 83us/step - loss: 3.0745e-04 - acc: 1.0000 - val_loss: 0.0828 - val_acc: 0.9803\n",
      "Epoch 162/1000\n",
      "1012/1012 [==============================] - 0s 86us/step - loss: 2.6408e-04 - acc: 1.0000 - val_loss: 0.0726 - val_acc: 0.9843\n",
      "Epoch 163/1000\n",
      "1012/1012 [==============================] - 0s 84us/step - loss: 2.6387e-04 - acc: 1.0000 - val_loss: 0.0839 - val_acc: 0.9803\n",
      "Epoch 164/1000\n",
      "1012/1012 [==============================] - 0s 82us/step - loss: 3.9006e-04 - acc: 1.0000 - val_loss: 0.0792 - val_acc: 0.9843\n",
      "Epoch 165/1000\n",
      "1012/1012 [==============================] - 0s 78us/step - loss: 2.6903e-04 - acc: 1.0000 - val_loss: 0.0805 - val_acc: 0.9843\n",
      "Epoch 166/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012/1012 [==============================] - 0s 80us/step - loss: 2.3274e-04 - acc: 1.0000 - val_loss: 0.0925 - val_acc: 0.9803\n",
      "Epoch 167/1000\n",
      "1012/1012 [==============================] - 0s 94us/step - loss: 2.6355e-04 - acc: 1.0000 - val_loss: 0.0877 - val_acc: 0.9803\n",
      "Epoch 168/1000\n",
      "1012/1012 [==============================] - 0s 95us/step - loss: 2.3999e-04 - acc: 1.0000 - val_loss: 0.0771 - val_acc: 0.9843\n",
      "Epoch 169/1000\n",
      "1012/1012 [==============================] - 0s 90us/step - loss: 2.4442e-04 - acc: 1.0000 - val_loss: 0.0840 - val_acc: 0.9803\n",
      "Epoch 170/1000\n",
      "1012/1012 [==============================] - 0s 75us/step - loss: 2.0911e-04 - acc: 1.0000 - val_loss: 0.0820 - val_acc: 0.9803\n",
      "Epoch 171/1000\n",
      "1012/1012 [==============================] - 0s 80us/step - loss: 2.0508e-04 - acc: 1.0000 - val_loss: 0.0843 - val_acc: 0.9803\n",
      "Epoch 172/1000\n",
      "1012/1012 [==============================] - 0s 88us/step - loss: 1.9570e-04 - acc: 1.0000 - val_loss: 0.0845 - val_acc: 0.9803\n",
      "Epoch 173/1000\n",
      "1012/1012 [==============================] - 0s 80us/step - loss: 2.0592e-04 - acc: 1.0000 - val_loss: 0.0822 - val_acc: 0.9803\n",
      "Epoch 174/1000\n",
      "1012/1012 [==============================] - 0s 95us/step - loss: 2.1389e-04 - acc: 1.0000 - val_loss: 0.0867 - val_acc: 0.9803\n",
      "Epoch 175/1000\n",
      "1012/1012 [==============================] - 0s 88us/step - loss: 1.8603e-04 - acc: 1.0000 - val_loss: 0.0892 - val_acc: 0.9803\n",
      "Epoch 176/1000\n",
      "1012/1012 [==============================] - 0s 87us/step - loss: 1.9261e-04 - acc: 1.0000 - val_loss: 0.0818 - val_acc: 0.9843\n",
      "Epoch 177/1000\n",
      "1012/1012 [==============================] - 0s 86us/step - loss: 2.2641e-04 - acc: 1.0000 - val_loss: 0.0831 - val_acc: 0.9843\n",
      "Epoch 178/1000\n",
      "1012/1012 [==============================] - 0s 77us/step - loss: 1.7228e-04 - acc: 1.0000 - val_loss: 0.0839 - val_acc: 0.9803\n",
      "Epoch 179/1000\n",
      "1012/1012 [==============================] - 0s 74us/step - loss: 3.2215e-04 - acc: 1.0000 - val_loss: 0.0770 - val_acc: 0.9843\n",
      "Epoch 180/1000\n",
      "1012/1012 [==============================] - 0s 82us/step - loss: 2.6378e-04 - acc: 1.0000 - val_loss: 0.0851 - val_acc: 0.9803\n",
      "Epoch 181/1000\n",
      "1012/1012 [==============================] - 0s 85us/step - loss: 1.9068e-04 - acc: 1.0000 - val_loss: 0.0820 - val_acc: 0.9843\n",
      "Epoch 182/1000\n",
      "1012/1012 [==============================] - 0s 89us/step - loss: 1.9104e-04 - acc: 1.0000 - val_loss: 0.0846 - val_acc: 0.9803\n",
      "Epoch 183/1000\n",
      "1012/1012 [==============================] - 0s 92us/step - loss: 2.2373e-04 - acc: 1.0000 - val_loss: 0.0886 - val_acc: 0.9803\n",
      "Epoch 184/1000\n",
      "1012/1012 [==============================] - 0s 88us/step - loss: 1.6869e-04 - acc: 1.0000 - val_loss: 0.0808 - val_acc: 0.9843\n",
      "Epoch 185/1000\n",
      "1012/1012 [==============================] - 0s 92us/step - loss: 1.7128e-04 - acc: 1.0000 - val_loss: 0.0820 - val_acc: 0.9843\n",
      "Epoch 186/1000\n",
      "1012/1012 [==============================] - 0s 91us/step - loss: 2.2343e-04 - acc: 1.0000 - val_loss: 0.0856 - val_acc: 0.9803\n",
      "Epoch 187/1000\n",
      "1012/1012 [==============================] - 0s 93us/step - loss: 1.7056e-04 - acc: 1.0000 - val_loss: 0.0834 - val_acc: 0.9843\n",
      "Epoch 188/1000\n",
      "1012/1012 [==============================] - 0s 95us/step - loss: 1.8767e-04 - acc: 1.0000 - val_loss: 0.0832 - val_acc: 0.9843\n",
      "Epoch 189/1000\n",
      "1012/1012 [==============================] - 0s 93us/step - loss: 1.8267e-04 - acc: 1.0000 - val_loss: 0.0832 - val_acc: 0.9843\n",
      "Epoch 190/1000\n",
      "1012/1012 [==============================] - 0s 90us/step - loss: 2.1876e-04 - acc: 1.0000 - val_loss: 0.0810 - val_acc: 0.9843\n",
      "Epoch 191/1000\n",
      "1012/1012 [==============================] - 0s 95us/step - loss: 1.7818e-04 - acc: 1.0000 - val_loss: 0.0825 - val_acc: 0.9843\n",
      "Epoch 192/1000\n",
      "1012/1012 [==============================] - 0s 98us/step - loss: 2.0822e-04 - acc: 1.0000 - val_loss: 0.0848 - val_acc: 0.9803\n",
      "Epoch 193/1000\n",
      "1012/1012 [==============================] - 0s 92us/step - loss: 1.7222e-04 - acc: 1.0000 - val_loss: 0.0821 - val_acc: 0.9843\n",
      "Epoch 194/1000\n",
      "1012/1012 [==============================] - 0s 92us/step - loss: 1.9771e-04 - acc: 1.0000 - val_loss: 0.0798 - val_acc: 0.9843\n",
      "Epoch 195/1000\n",
      "1012/1012 [==============================] - 0s 93us/step - loss: 1.6164e-04 - acc: 1.0000 - val_loss: 0.0831 - val_acc: 0.9843\n",
      "Epoch 196/1000\n",
      "1012/1012 [==============================] - 0s 93us/step - loss: 1.8172e-04 - acc: 1.0000 - val_loss: 0.0847 - val_acc: 0.9803\n",
      "Epoch 00196: early stopping\n"
     ]
    }
   ],
   "source": [
    "#HUMAN OBSERVED DATASET CONCATENATED\n",
    "RawData, RawTarget = Select_HODS(Final_HODS, Final_SHODS, Concatenated = True)\n",
    "#print(RawData.shape, RawTarget.shape)\n",
    "TrainingData, TrainingTarget, ValData, ValDataAct, TestData, TestDataAct = Generate_Data_Split(RawData, RawTarget)\n",
    "model = get_model(TrainingData.shape[0])\n",
    "history = Neural_Net(TrainingData, TrainingTarget)\n",
    "\n",
    "NN_Graph(history)\n",
    "\n",
    "NN_Accuracy(TestData, TestDataAct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Observed Dataset with Subtraction selected\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 256)               2560      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 513       \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 134,657\n",
      "Trainable params: 134,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1012 samples, validate on 254 samples\n",
      "Epoch 1/1000\n",
      "1012/1012 [==============================] - 1s 608us/step - loss: 0.6364 - acc: 0.5800 - val_loss: 0.5330 - val_acc: 0.7362\n",
      "Epoch 2/1000\n",
      "1012/1012 [==============================] - 0s 73us/step - loss: 0.5487 - acc: 0.7302 - val_loss: 0.4610 - val_acc: 0.7953\n",
      "Epoch 3/1000\n",
      "1012/1012 [==============================] - 0s 81us/step - loss: 0.4927 - acc: 0.7708 - val_loss: 0.4221 - val_acc: 0.8386\n",
      "Epoch 4/1000\n",
      "1012/1012 [==============================] - 0s 80us/step - loss: 0.4468 - acc: 0.8202 - val_loss: 0.3750 - val_acc: 0.8307\n",
      "Epoch 5/1000\n",
      "1012/1012 [==============================] - 0s 70us/step - loss: 0.4143 - acc: 0.8360 - val_loss: 0.3502 - val_acc: 0.8504\n",
      "Epoch 6/1000\n",
      "1012/1012 [==============================] - 0s 80us/step - loss: 0.3908 - acc: 0.8567 - val_loss: 0.3320 - val_acc: 0.8583\n",
      "Epoch 7/1000\n",
      "1012/1012 [==============================] - 0s 87us/step - loss: 0.3698 - acc: 0.8577 - val_loss: 0.3173 - val_acc: 0.8583\n",
      "Epoch 8/1000\n",
      "1012/1012 [==============================] - 0s 88us/step - loss: 0.3573 - acc: 0.8587 - val_loss: 0.3299 - val_acc: 0.8425\n",
      "Epoch 9/1000\n",
      "1012/1012 [==============================] - 0s 96us/step - loss: 0.3399 - acc: 0.8745 - val_loss: 0.3090 - val_acc: 0.8661\n",
      "Epoch 10/1000\n",
      "1012/1012 [==============================] - 0s 93us/step - loss: 0.3405 - acc: 0.8607 - val_loss: 0.3036 - val_acc: 0.8661\n",
      "Epoch 11/1000\n",
      "1012/1012 [==============================] - 0s 94us/step - loss: 0.3231 - acc: 0.8715 - val_loss: 0.3021 - val_acc: 0.8504\n",
      "Epoch 12/1000\n",
      "1012/1012 [==============================] - 0s 85us/step - loss: 0.3166 - acc: 0.8745 - val_loss: 0.3048 - val_acc: 0.8465\n",
      "Epoch 13/1000\n",
      "1012/1012 [==============================] - 0s 79us/step - loss: 0.3193 - acc: 0.8666 - val_loss: 0.3032 - val_acc: 0.8504\n",
      "Epoch 14/1000\n",
      "1012/1012 [==============================] - 0s 81us/step - loss: 0.3069 - acc: 0.8785 - val_loss: 0.2988 - val_acc: 0.8701\n",
      "Epoch 15/1000\n",
      "1012/1012 [==============================] - 0s 78us/step - loss: 0.3040 - acc: 0.8765 - val_loss: 0.2957 - val_acc: 0.8543\n",
      "Epoch 16/1000\n",
      "1012/1012 [==============================] - 0s 85us/step - loss: 0.2906 - acc: 0.8893 - val_loss: 0.2914 - val_acc: 0.8583\n",
      "Epoch 17/1000\n",
      "1012/1012 [==============================] - 0s 82us/step - loss: 0.2876 - acc: 0.8923 - val_loss: 0.2957 - val_acc: 0.8543\n",
      "Epoch 18/1000\n",
      "1012/1012 [==============================] - 0s 80us/step - loss: 0.2864 - acc: 0.8804 - val_loss: 0.3047 - val_acc: 0.8583\n",
      "Epoch 19/1000\n",
      "1012/1012 [==============================] - 0s 79us/step - loss: 0.2784 - acc: 0.8785 - val_loss: 0.2924 - val_acc: 0.8661\n",
      "Epoch 20/1000\n",
      "1012/1012 [==============================] - 0s 76us/step - loss: 0.2720 - acc: 0.8913 - val_loss: 0.2935 - val_acc: 0.8701\n",
      "Epoch 21/1000\n",
      "1012/1012 [==============================] - 0s 81us/step - loss: 0.2710 - acc: 0.8992 - val_loss: 0.3003 - val_acc: 0.8465\n",
      "Epoch 22/1000\n",
      "1012/1012 [==============================] - 0s 78us/step - loss: 0.2800 - acc: 0.8923 - val_loss: 0.2939 - val_acc: 0.8543\n",
      "Epoch 23/1000\n",
      "1012/1012 [==============================] - 0s 82us/step - loss: 0.2720 - acc: 0.8844 - val_loss: 0.3069 - val_acc: 0.8583\n",
      "Epoch 24/1000\n",
      "1012/1012 [==============================] - 0s 86us/step - loss: 0.2672 - acc: 0.8854 - val_loss: 0.2964 - val_acc: 0.8701\n",
      "Epoch 25/1000\n",
      "1012/1012 [==============================] - 0s 87us/step - loss: 0.2578 - acc: 0.8962 - val_loss: 0.2932 - val_acc: 0.8701\n",
      "Epoch 26/1000\n",
      "1012/1012 [==============================] - 0s 80us/step - loss: 0.2534 - acc: 0.8933 - val_loss: 0.2914 - val_acc: 0.8622\n",
      "Epoch 27/1000\n",
      "1012/1012 [==============================] - 0s 82us/step - loss: 0.2522 - acc: 0.8972 - val_loss: 0.2934 - val_acc: 0.8701\n",
      "Epoch 28/1000\n",
      "1012/1012 [==============================] - 0s 75us/step - loss: 0.2481 - acc: 0.8953 - val_loss: 0.2958 - val_acc: 0.8583\n",
      "Epoch 29/1000\n",
      "1012/1012 [==============================] - 0s 78us/step - loss: 0.2430 - acc: 0.9002 - val_loss: 0.2950 - val_acc: 0.8583\n",
      "Epoch 30/1000\n",
      "1012/1012 [==============================] - 0s 78us/step - loss: 0.2436 - acc: 0.8982 - val_loss: 0.2988 - val_acc: 0.8740\n",
      "Epoch 31/1000\n",
      "1012/1012 [==============================] - 0s 79us/step - loss: 0.2441 - acc: 0.8933 - val_loss: 0.2979 - val_acc: 0.8543\n",
      "Epoch 32/1000\n",
      "1012/1012 [==============================] - 0s 80us/step - loss: 0.2405 - acc: 0.9061 - val_loss: 0.3011 - val_acc: 0.8543\n",
      "Epoch 33/1000\n",
      "1012/1012 [==============================] - 0s 79us/step - loss: 0.2420 - acc: 0.9032 - val_loss: 0.3171 - val_acc: 0.8701\n",
      "Epoch 34/1000\n",
      "1012/1012 [==============================] - 0s 74us/step - loss: 0.2396 - acc: 0.8923 - val_loss: 0.2951 - val_acc: 0.8661\n",
      "Epoch 35/1000\n",
      "1012/1012 [==============================] - 0s 79us/step - loss: 0.2333 - acc: 0.9032 - val_loss: 0.3009 - val_acc: 0.8622\n",
      "Epoch 36/1000\n",
      "1012/1012 [==============================] - 0s 98us/step - loss: 0.2272 - acc: 0.9101 - val_loss: 0.3008 - val_acc: 0.8543\n",
      "Epoch 37/1000\n",
      "1012/1012 [==============================] - 0s 92us/step - loss: 0.2267 - acc: 0.9071 - val_loss: 0.3044 - val_acc: 0.8543\n",
      "Epoch 38/1000\n",
      "1012/1012 [==============================] - 0s 95us/step - loss: 0.2327 - acc: 0.9042 - val_loss: 0.3087 - val_acc: 0.8701\n",
      "Epoch 39/1000\n",
      "1012/1012 [==============================] - 0s 87us/step - loss: 0.2185 - acc: 0.9121 - val_loss: 0.3078 - val_acc: 0.8583\n",
      "Epoch 40/1000\n",
      "1012/1012 [==============================] - 0s 81us/step - loss: 0.2256 - acc: 0.9061 - val_loss: 0.3080 - val_acc: 0.8661\n",
      "Epoch 41/1000\n",
      "1012/1012 [==============================] - 0s 80us/step - loss: 0.2198 - acc: 0.9051 - val_loss: 0.3082 - val_acc: 0.8622\n",
      "Epoch 42/1000\n",
      "1012/1012 [==============================] - 0s 81us/step - loss: 0.2186 - acc: 0.9101 - val_loss: 0.3079 - val_acc: 0.8701\n",
      "Epoch 43/1000\n",
      "1012/1012 [==============================] - 0s 102us/step - loss: 0.2178 - acc: 0.9111 - val_loss: 0.3044 - val_acc: 0.8622\n",
      "Epoch 44/1000\n",
      "1012/1012 [==============================] - 0s 85us/step - loss: 0.2167 - acc: 0.9101 - val_loss: 0.3154 - val_acc: 0.8425\n",
      "Epoch 45/1000\n",
      "1012/1012 [==============================] - 0s 93us/step - loss: 0.2093 - acc: 0.9190 - val_loss: 0.3099 - val_acc: 0.8622\n",
      "Epoch 46/1000\n",
      "1012/1012 [==============================] - 0s 100us/step - loss: 0.2140 - acc: 0.9032 - val_loss: 0.3196 - val_acc: 0.8622\n",
      "Epoch 47/1000\n",
      "1012/1012 [==============================] - 0s 95us/step - loss: 0.2107 - acc: 0.9170 - val_loss: 0.3166 - val_acc: 0.8622\n",
      "Epoch 48/1000\n",
      "1012/1012 [==============================] - 0s 98us/step - loss: 0.2080 - acc: 0.9121 - val_loss: 0.3185 - val_acc: 0.8543\n",
      "Epoch 49/1000\n",
      "1012/1012 [==============================] - 0s 95us/step - loss: 0.2071 - acc: 0.9121 - val_loss: 0.3168 - val_acc: 0.8583\n",
      "Epoch 50/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012/1012 [==============================] - 0s 102us/step - loss: 0.2062 - acc: 0.9180 - val_loss: 0.3292 - val_acc: 0.8465\n",
      "Epoch 51/1000\n",
      "1012/1012 [==============================] - 0s 96us/step - loss: 0.2028 - acc: 0.9180 - val_loss: 0.3203 - val_acc: 0.8622\n",
      "Epoch 52/1000\n",
      "1012/1012 [==============================] - 0s 99us/step - loss: 0.2041 - acc: 0.9130 - val_loss: 0.3318 - val_acc: 0.8622\n",
      "Epoch 53/1000\n",
      "1012/1012 [==============================] - 0s 101us/step - loss: 0.2037 - acc: 0.9150 - val_loss: 0.3302 - val_acc: 0.8622\n",
      "Epoch 54/1000\n",
      "1012/1012 [==============================] - 0s 102us/step - loss: 0.2024 - acc: 0.9160 - val_loss: 0.3295 - val_acc: 0.8622\n",
      "Epoch 55/1000\n",
      "1012/1012 [==============================] - 0s 94us/step - loss: 0.1966 - acc: 0.9190 - val_loss: 0.3308 - val_acc: 0.8504\n",
      "Epoch 56/1000\n",
      "1012/1012 [==============================] - 0s 101us/step - loss: 0.1950 - acc: 0.9150 - val_loss: 0.3298 - val_acc: 0.8543\n",
      "Epoch 57/1000\n",
      "1012/1012 [==============================] - 0s 86us/step - loss: 0.1965 - acc: 0.9170 - val_loss: 0.3358 - val_acc: 0.8465\n",
      "Epoch 58/1000\n",
      "1012/1012 [==============================] - 0s 96us/step - loss: 0.1883 - acc: 0.9239 - val_loss: 0.3383 - val_acc: 0.8622\n",
      "Epoch 59/1000\n",
      "1012/1012 [==============================] - 0s 86us/step - loss: 0.1948 - acc: 0.9190 - val_loss: 0.3352 - val_acc: 0.8504\n",
      "Epoch 60/1000\n",
      "1012/1012 [==============================] - 0s 81us/step - loss: 0.1945 - acc: 0.9170 - val_loss: 0.3399 - val_acc: 0.8543\n",
      "Epoch 61/1000\n",
      "1012/1012 [==============================] - 0s 84us/step - loss: 0.1908 - acc: 0.9190 - val_loss: 0.3487 - val_acc: 0.8425\n",
      "Epoch 62/1000\n",
      "1012/1012 [==============================] - 0s 92us/step - loss: 0.1876 - acc: 0.9160 - val_loss: 0.3444 - val_acc: 0.8543\n",
      "Epoch 63/1000\n",
      "1012/1012 [==============================] - 0s 91us/step - loss: 0.1864 - acc: 0.9259 - val_loss: 0.3631 - val_acc: 0.8465\n",
      "Epoch 64/1000\n",
      "1012/1012 [==============================] - 0s 94us/step - loss: 0.1890 - acc: 0.9150 - val_loss: 0.3501 - val_acc: 0.8661\n",
      "Epoch 65/1000\n",
      "1012/1012 [==============================] - 0s 95us/step - loss: 0.1890 - acc: 0.9130 - val_loss: 0.3482 - val_acc: 0.8465\n",
      "Epoch 66/1000\n",
      "1012/1012 [==============================] - 0s 85us/step - loss: 0.1824 - acc: 0.9190 - val_loss: 0.3558 - val_acc: 0.8465\n",
      "Epoch 67/1000\n",
      "1012/1012 [==============================] - 0s 85us/step - loss: 0.1841 - acc: 0.9229 - val_loss: 0.3623 - val_acc: 0.8583\n",
      "Epoch 68/1000\n",
      "1012/1012 [==============================] - 0s 91us/step - loss: 0.1928 - acc: 0.9170 - val_loss: 0.3676 - val_acc: 0.8661\n",
      "Epoch 69/1000\n",
      "1012/1012 [==============================] - 0s 85us/step - loss: 0.1954 - acc: 0.9160 - val_loss: 0.3692 - val_acc: 0.8465\n",
      "Epoch 70/1000\n",
      "1012/1012 [==============================] - 0s 94us/step - loss: 0.1833 - acc: 0.9209 - val_loss: 0.3609 - val_acc: 0.8583\n",
      "Epoch 71/1000\n",
      "1012/1012 [==============================] - 0s 95us/step - loss: 0.1846 - acc: 0.9209 - val_loss: 0.3661 - val_acc: 0.8504\n",
      "Epoch 72/1000\n",
      "1012/1012 [==============================] - 0s 88us/step - loss: 0.1946 - acc: 0.9071 - val_loss: 0.3974 - val_acc: 0.8622\n",
      "Epoch 73/1000\n",
      "1012/1012 [==============================] - 0s 94us/step - loss: 0.1900 - acc: 0.9239 - val_loss: 0.3626 - val_acc: 0.8543\n",
      "Epoch 74/1000\n",
      "1012/1012 [==============================] - 0s 90us/step - loss: 0.1815 - acc: 0.9219 - val_loss: 0.3639 - val_acc: 0.8543\n",
      "Epoch 75/1000\n",
      "1012/1012 [==============================] - 0s 95us/step - loss: 0.1841 - acc: 0.9209 - val_loss: 0.3690 - val_acc: 0.8465\n",
      "Epoch 76/1000\n",
      "1012/1012 [==============================] - 0s 91us/step - loss: 0.1768 - acc: 0.9259 - val_loss: 0.3659 - val_acc: 0.8465\n",
      "Epoch 77/1000\n",
      "1012/1012 [==============================] - 0s 84us/step - loss: 0.1721 - acc: 0.9239 - val_loss: 0.3868 - val_acc: 0.8346\n",
      "Epoch 78/1000\n",
      "1012/1012 [==============================] - 0s 99us/step - loss: 0.1742 - acc: 0.9318 - val_loss: 0.3674 - val_acc: 0.8583\n",
      "Epoch 79/1000\n",
      "1012/1012 [==============================] - 0s 90us/step - loss: 0.1742 - acc: 0.9239 - val_loss: 0.3752 - val_acc: 0.8622\n",
      "Epoch 80/1000\n",
      "1012/1012 [==============================] - 0s 94us/step - loss: 0.1734 - acc: 0.9229 - val_loss: 0.3758 - val_acc: 0.8622\n",
      "Epoch 81/1000\n",
      "1012/1012 [==============================] - 0s 92us/step - loss: 0.1680 - acc: 0.9328 - val_loss: 0.4006 - val_acc: 0.8346\n",
      "Epoch 82/1000\n",
      "1012/1012 [==============================] - 0s 82us/step - loss: 0.1758 - acc: 0.9229 - val_loss: 0.3790 - val_acc: 0.8543\n",
      "Epoch 83/1000\n",
      "1012/1012 [==============================] - 0s 99us/step - loss: 0.1732 - acc: 0.9229 - val_loss: 0.3964 - val_acc: 0.8661\n",
      "Epoch 84/1000\n",
      "1012/1012 [==============================] - 0s 85us/step - loss: 0.1747 - acc: 0.9259 - val_loss: 0.3792 - val_acc: 0.8425\n",
      "Epoch 85/1000\n",
      "1012/1012 [==============================] - 0s 105us/step - loss: 0.1652 - acc: 0.9269 - val_loss: 0.3894 - val_acc: 0.8504\n",
      "Epoch 86/1000\n",
      "1012/1012 [==============================] - 0s 95us/step - loss: 0.1643 - acc: 0.9239 - val_loss: 0.3956 - val_acc: 0.8583\n",
      "Epoch 87/1000\n",
      "1012/1012 [==============================] - 0s 84us/step - loss: 0.1776 - acc: 0.9190 - val_loss: 0.4007 - val_acc: 0.8504\n",
      "Epoch 88/1000\n",
      "1012/1012 [==============================] - 0s 93us/step - loss: 0.1618 - acc: 0.9298 - val_loss: 0.3905 - val_acc: 0.8465\n",
      "Epoch 89/1000\n",
      "1012/1012 [==============================] - 0s 89us/step - loss: 0.1687 - acc: 0.9259 - val_loss: 0.4020 - val_acc: 0.8425\n",
      "Epoch 90/1000\n",
      "1012/1012 [==============================] - 0s 90us/step - loss: 0.1685 - acc: 0.9298 - val_loss: 0.4112 - val_acc: 0.8701\n",
      "Epoch 91/1000\n",
      "1012/1012 [==============================] - 0s 95us/step - loss: 0.1699 - acc: 0.9298 - val_loss: 0.3970 - val_acc: 0.8543\n",
      "Epoch 92/1000\n",
      "1012/1012 [==============================] - 0s 88us/step - loss: 0.1642 - acc: 0.9279 - val_loss: 0.4087 - val_acc: 0.8425\n",
      "Epoch 93/1000\n",
      "1012/1012 [==============================] - 0s 101us/step - loss: 0.1687 - acc: 0.9259 - val_loss: 0.4055 - val_acc: 0.8543\n",
      "Epoch 94/1000\n",
      "1012/1012 [==============================] - 0s 87us/step - loss: 0.1700 - acc: 0.9259 - val_loss: 0.4053 - val_acc: 0.8465\n",
      "Epoch 95/1000\n",
      "1012/1012 [==============================] - 0s 96us/step - loss: 0.1643 - acc: 0.9279 - val_loss: 0.4009 - val_acc: 0.8504\n",
      "Epoch 96/1000\n",
      "1012/1012 [==============================] - 0s 86us/step - loss: 0.1607 - acc: 0.9269 - val_loss: 0.4096 - val_acc: 0.8465\n",
      "Epoch 97/1000\n",
      "1012/1012 [==============================] - 0s 85us/step - loss: 0.1603 - acc: 0.9239 - val_loss: 0.4174 - val_acc: 0.8346\n",
      "Epoch 98/1000\n",
      "1012/1012 [==============================] - 0s 87us/step - loss: 0.1657 - acc: 0.9259 - val_loss: 0.4175 - val_acc: 0.8543\n",
      "Epoch 99/1000\n",
      "1012/1012 [==============================] - 0s 87us/step - loss: 0.1666 - acc: 0.9269 - val_loss: 0.4197 - val_acc: 0.8622\n",
      "Epoch 100/1000\n",
      "1012/1012 [==============================] - 0s 93us/step - loss: 0.1625 - acc: 0.9298 - val_loss: 0.4118 - val_acc: 0.8504\n",
      "Epoch 101/1000\n",
      "1012/1012 [==============================] - 0s 96us/step - loss: 0.1640 - acc: 0.9289 - val_loss: 0.4267 - val_acc: 0.8268\n",
      "Epoch 102/1000\n",
      "1012/1012 [==============================] - 0s 84us/step - loss: 0.1574 - acc: 0.9308 - val_loss: 0.4197 - val_acc: 0.8504\n",
      "Epoch 103/1000\n",
      "1012/1012 [==============================] - 0s 95us/step - loss: 0.1569 - acc: 0.9358 - val_loss: 0.4156 - val_acc: 0.8386\n",
      "Epoch 104/1000\n",
      "1012/1012 [==============================] - 0s 91us/step - loss: 0.1605 - acc: 0.9298 - val_loss: 0.4233 - val_acc: 0.8543\n",
      "Epoch 105/1000\n",
      "1012/1012 [==============================] - 0s 97us/step - loss: 0.1648 - acc: 0.9249 - val_loss: 0.4253 - val_acc: 0.8622\n",
      "Epoch 106/1000\n",
      "1012/1012 [==============================] - 0s 96us/step - loss: 0.1592 - acc: 0.9239 - val_loss: 0.4307 - val_acc: 0.8425\n",
      "Epoch 107/1000\n",
      "1012/1012 [==============================] - 0s 88us/step - loss: 0.1561 - acc: 0.9328 - val_loss: 0.4188 - val_acc: 0.8465\n",
      "Epoch 108/1000\n",
      "1012/1012 [==============================] - 0s 84us/step - loss: 0.1618 - acc: 0.9279 - val_loss: 0.4558 - val_acc: 0.8583\n",
      "Epoch 109/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1012/1012 [==============================] - 0s 79us/step - loss: 0.1585 - acc: 0.9338 - val_loss: 0.4422 - val_acc: 0.8346\n",
      "Epoch 110/1000\n",
      "1012/1012 [==============================] - 0s 86us/step - loss: 0.1599 - acc: 0.9269 - val_loss: 0.4355 - val_acc: 0.8386\n",
      "Epoch 111/1000\n",
      "1012/1012 [==============================] - 0s 94us/step - loss: 0.1600 - acc: 0.9279 - val_loss: 0.4378 - val_acc: 0.8425\n",
      "Epoch 112/1000\n",
      "1012/1012 [==============================] - 0s 101us/step - loss: 0.1646 - acc: 0.9249 - val_loss: 0.4369 - val_acc: 0.8386\n",
      "Epoch 113/1000\n",
      "1012/1012 [==============================] - 0s 87us/step - loss: 0.1602 - acc: 0.9338 - val_loss: 0.4444 - val_acc: 0.8425\n",
      "Epoch 114/1000\n",
      "1012/1012 [==============================] - 0s 97us/step - loss: 0.1724 - acc: 0.9229 - val_loss: 0.4229 - val_acc: 0.8346\n",
      "Epoch 115/1000\n",
      "1012/1012 [==============================] - 0s 96us/step - loss: 0.1507 - acc: 0.9377 - val_loss: 0.4379 - val_acc: 0.8583\n",
      "Epoch 116/1000\n",
      "1012/1012 [==============================] - 0s 100us/step - loss: 0.1615 - acc: 0.9269 - val_loss: 0.4474 - val_acc: 0.8701\n",
      "Epoch 00116: early stopping\n"
     ]
    }
   ],
   "source": [
    "#HUMAN OBSERVED DATASET SUBTRACTED\n",
    "RawData, RawTarget = Select_HODS(Final_HODS, Final_SHODS, Concatenated = False)\n",
    "#print(RawData.shape, RawTarget.shape)\n",
    "TrainingData, TrainingTarget, ValData, ValDataAct, TestData, TestDataAct = Generate_Data_Split(RawData, RawTarget)\n",
    "model = get_model(TrainingData.shape[0])\n",
    "history = Neural_Net(TrainingData, TrainingTarget)\n",
    "\n",
    "NN_Graph(history)\n",
    "NN_Accuracy(TestData, TestDataAct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSC Dataset with feature concatenation\n",
      "(940, 5000)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 256)               240896    \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 1)                 513       \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 372,993\n",
      "Trainable params: 372,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/1000\n",
      "3200/3200 [==============================] - 1s 305us/step - loss: 0.2230 - acc: 0.9134 - val_loss: 0.0815 - val_acc: 0.9838\n",
      "Epoch 2/1000\n",
      "3200/3200 [==============================] - 0s 94us/step - loss: 0.0499 - acc: 0.9884 - val_loss: 0.0536 - val_acc: 0.9862\n",
      "Epoch 3/1000\n",
      "3200/3200 [==============================] - 0s 99us/step - loss: 0.0339 - acc: 0.9884 - val_loss: 0.0571 - val_acc: 0.9862\n",
      "Epoch 4/1000\n",
      "3200/3200 [==============================] - 0s 101us/step - loss: 0.0266 - acc: 0.9909 - val_loss: 0.0619 - val_acc: 0.9862\n",
      "Epoch 5/1000\n",
      "3200/3200 [==============================] - 0s 105us/step - loss: 0.0173 - acc: 0.9941 - val_loss: 0.0568 - val_acc: 0.9812\n",
      "Epoch 6/1000\n",
      "3200/3200 [==============================] - 0s 102us/step - loss: 0.0081 - acc: 0.9981 - val_loss: 0.0645 - val_acc: 0.9875\n",
      "Epoch 7/1000\n",
      "3200/3200 [==============================] - 0s 109us/step - loss: 0.0041 - acc: 0.9991 - val_loss: 0.0729 - val_acc: 0.9838\n",
      "Epoch 8/1000\n",
      "3200/3200 [==============================] - 0s 111us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0844 - val_acc: 0.9862\n",
      "Epoch 9/1000\n",
      "3200/3200 [==============================] - 0s 103us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0861 - val_acc: 0.9862\n",
      "Epoch 10/1000\n",
      "3200/3200 [==============================] - 0s 101us/step - loss: 6.3965e-04 - acc: 1.0000 - val_loss: 0.0870 - val_acc: 0.9862\n",
      "Epoch 11/1000\n",
      "3200/3200 [==============================] - 0s 114us/step - loss: 4.2702e-04 - acc: 1.0000 - val_loss: 0.0878 - val_acc: 0.9875\n",
      "Epoch 12/1000\n",
      "3200/3200 [==============================] - 0s 111us/step - loss: 3.3599e-04 - acc: 1.0000 - val_loss: 0.0876 - val_acc: 0.9862\n",
      "Epoch 13/1000\n",
      "3200/3200 [==============================] - 0s 106us/step - loss: 2.7308e-04 - acc: 1.0000 - val_loss: 0.0893 - val_acc: 0.9875\n",
      "Epoch 14/1000\n",
      "3200/3200 [==============================] - 0s 109us/step - loss: 2.1692e-04 - acc: 1.0000 - val_loss: 0.0877 - val_acc: 0.9875\n",
      "Epoch 15/1000\n",
      "3200/3200 [==============================] - 0s 116us/step - loss: 1.8163e-04 - acc: 1.0000 - val_loss: 0.0931 - val_acc: 0.9862\n",
      "Epoch 16/1000\n",
      "3200/3200 [==============================] - 0s 122us/step - loss: 1.3153e-04 - acc: 1.0000 - val_loss: 0.0913 - val_acc: 0.9875\n",
      "Epoch 17/1000\n",
      "3200/3200 [==============================] - 0s 104us/step - loss: 1.2164e-04 - acc: 1.0000 - val_loss: 0.0961 - val_acc: 0.9875\n",
      "Epoch 18/1000\n",
      "3200/3200 [==============================] - 0s 99us/step - loss: 1.0981e-04 - acc: 1.0000 - val_loss: 0.0959 - val_acc: 0.9875\n",
      "Epoch 19/1000\n",
      "3200/3200 [==============================] - 0s 103us/step - loss: 8.6429e-05 - acc: 1.0000 - val_loss: 0.0979 - val_acc: 0.9875\n",
      "Epoch 20/1000\n",
      "3200/3200 [==============================] - 0s 109us/step - loss: 8.5792e-05 - acc: 1.0000 - val_loss: 0.0982 - val_acc: 0.9875\n",
      "Epoch 21/1000\n",
      "3200/3200 [==============================] - 0s 105us/step - loss: 6.2979e-05 - acc: 1.0000 - val_loss: 0.1036 - val_acc: 0.9862\n",
      "Epoch 22/1000\n",
      "3200/3200 [==============================] - 0s 101us/step - loss: 8.1966e-05 - acc: 1.0000 - val_loss: 0.1011 - val_acc: 0.9862\n",
      "Epoch 23/1000\n",
      "3200/3200 [==============================] - 0s 109us/step - loss: 5.7412e-05 - acc: 1.0000 - val_loss: 0.0972 - val_acc: 0.9862\n",
      "Epoch 24/1000\n",
      "3200/3200 [==============================] - 0s 119us/step - loss: 5.2325e-05 - acc: 1.0000 - val_loss: 0.1019 - val_acc: 0.9875\n",
      "Epoch 25/1000\n",
      "3200/3200 [==============================] - 0s 100us/step - loss: 5.0119e-05 - acc: 1.0000 - val_loss: 0.1017 - val_acc: 0.9862\n",
      "Epoch 26/1000\n",
      "3200/3200 [==============================] - 0s 101us/step - loss: 4.2050e-05 - acc: 1.0000 - val_loss: 0.1023 - val_acc: 0.9862\n",
      "Epoch 27/1000\n",
      "3200/3200 [==============================] - 0s 114us/step - loss: 3.8496e-05 - acc: 1.0000 - val_loss: 0.1040 - val_acc: 0.9875\n",
      "Epoch 28/1000\n",
      "3200/3200 [==============================] - 0s 104us/step - loss: 3.4556e-05 - acc: 1.0000 - val_loss: 0.1033 - val_acc: 0.9875\n",
      "Epoch 29/1000\n",
      "3200/3200 [==============================] - 0s 100us/step - loss: 2.9865e-05 - acc: 1.0000 - val_loss: 0.1060 - val_acc: 0.9875\n",
      "Epoch 30/1000\n",
      "3200/3200 [==============================] - 0s 107us/step - loss: 2.9963e-05 - acc: 1.0000 - val_loss: 0.1026 - val_acc: 0.9862\n",
      "Epoch 31/1000\n",
      "3200/3200 [==============================] - 0s 104us/step - loss: 2.6257e-05 - acc: 1.0000 - val_loss: 0.1057 - val_acc: 0.9862\n",
      "Epoch 32/1000\n",
      "3200/3200 [==============================] - 0s 114us/step - loss: 2.3451e-05 - acc: 1.0000 - val_loss: 0.1070 - val_acc: 0.9875\n",
      "Epoch 33/1000\n",
      "3200/3200 [==============================] - 0s 121us/step - loss: 2.8383e-05 - acc: 1.0000 - val_loss: 0.1062 - val_acc: 0.9875\n",
      "Epoch 34/1000\n",
      "3200/3200 [==============================] - 0s 121us/step - loss: 2.2440e-05 - acc: 1.0000 - val_loss: 0.1076 - val_acc: 0.9875\n",
      "Epoch 35/1000\n",
      "3200/3200 [==============================] - 0s 122us/step - loss: 2.1851e-05 - acc: 1.0000 - val_loss: 0.1054 - val_acc: 0.9862\n",
      "Epoch 36/1000\n",
      "3200/3200 [==============================] - 0s 107us/step - loss: 1.9582e-05 - acc: 1.0000 - val_loss: 0.1083 - val_acc: 0.9862\n",
      "Epoch 37/1000\n",
      "3200/3200 [==============================] - 0s 120us/step - loss: 2.1230e-05 - acc: 1.0000 - val_loss: 0.1107 - val_acc: 0.9862\n",
      "Epoch 38/1000\n",
      "3200/3200 [==============================] - 0s 107us/step - loss: 1.7916e-05 - acc: 1.0000 - val_loss: 0.1096 - val_acc: 0.9875\n",
      "Epoch 39/1000\n",
      "3200/3200 [==============================] - 0s 97us/step - loss: 1.7339e-05 - acc: 1.0000 - val_loss: 0.1102 - val_acc: 0.9875\n",
      "Epoch 40/1000\n",
      "3200/3200 [==============================] - 0s 103us/step - loss: 1.2325e-05 - acc: 1.0000 - val_loss: 0.1117 - val_acc: 0.9875\n",
      "Epoch 41/1000\n",
      "3200/3200 [==============================] - 0s 97us/step - loss: 1.3196e-05 - acc: 1.0000 - val_loss: 0.1112 - val_acc: 0.9875\n",
      "Epoch 42/1000\n",
      "3200/3200 [==============================] - 0s 99us/step - loss: 1.5477e-05 - acc: 1.0000 - val_loss: 0.1107 - val_acc: 0.9875\n",
      "Epoch 43/1000\n",
      "3200/3200 [==============================] - 0s 100us/step - loss: 1.1943e-05 - acc: 1.0000 - val_loss: 0.1133 - val_acc: 0.9875\n",
      "Epoch 44/1000\n",
      "3200/3200 [==============================] - 0s 98us/step - loss: 1.0126e-05 - acc: 1.0000 - val_loss: 0.1136 - val_acc: 0.9875\n",
      "Epoch 45/1000\n",
      "3200/3200 [==============================] - 0s 110us/step - loss: 1.2471e-05 - acc: 1.0000 - val_loss: 0.1134 - val_acc: 0.9875\n",
      "Epoch 46/1000\n",
      "3200/3200 [==============================] - 0s 109us/step - loss: 1.5136e-05 - acc: 1.0000 - val_loss: 0.1073 - val_acc: 0.9862\n",
      "Epoch 47/1000\n",
      "3200/3200 [==============================] - 0s 108us/step - loss: 1.2339e-05 - acc: 1.0000 - val_loss: 0.1117 - val_acc: 0.9875\n",
      "Epoch 48/1000\n",
      "3200/3200 [==============================] - 0s 112us/step - loss: 1.1289e-05 - acc: 1.0000 - val_loss: 0.1130 - val_acc: 0.9875\n",
      "Epoch 49/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - 0s 122us/step - loss: 1.3999e-05 - acc: 1.0000 - val_loss: 0.1090 - val_acc: 0.9875\n",
      "Epoch 50/1000\n",
      "3200/3200 [==============================] - 0s 118us/step - loss: 8.5258e-06 - acc: 1.0000 - val_loss: 0.1159 - val_acc: 0.9875\n",
      "Epoch 51/1000\n",
      "3200/3200 [==============================] - 0s 98us/step - loss: 7.7165e-06 - acc: 1.0000 - val_loss: 0.1156 - val_acc: 0.9875\n",
      "Epoch 52/1000\n",
      "3200/3200 [==============================] - 0s 108us/step - loss: 6.8777e-06 - acc: 1.0000 - val_loss: 0.1174 - val_acc: 0.9875\n",
      "Epoch 53/1000\n",
      "3200/3200 [==============================] - 0s 109us/step - loss: 7.9532e-06 - acc: 1.0000 - val_loss: 0.1159 - val_acc: 0.9875\n",
      "Epoch 54/1000\n",
      "3200/3200 [==============================] - 0s 104us/step - loss: 1.0023e-05 - acc: 1.0000 - val_loss: 0.1134 - val_acc: 0.9875\n",
      "Epoch 55/1000\n",
      "3200/3200 [==============================] - 0s 99us/step - loss: 8.7530e-06 - acc: 1.0000 - val_loss: 0.1145 - val_acc: 0.9875\n",
      "Epoch 56/1000\n",
      "3200/3200 [==============================] - 0s 107us/step - loss: 8.4701e-06 - acc: 1.0000 - val_loss: 0.1175 - val_acc: 0.9875\n",
      "Epoch 57/1000\n",
      "3200/3200 [==============================] - 0s 105us/step - loss: 7.5828e-06 - acc: 1.0000 - val_loss: 0.1158 - val_acc: 0.9875\n",
      "Epoch 58/1000\n",
      "3200/3200 [==============================] - 0s 100us/step - loss: 7.2026e-06 - acc: 1.0000 - val_loss: 0.1160 - val_acc: 0.9862\n",
      "Epoch 59/1000\n",
      "3200/3200 [==============================] - 0s 101us/step - loss: 8.1010e-06 - acc: 1.0000 - val_loss: 0.1169 - val_acc: 0.9862\n",
      "Epoch 60/1000\n",
      "3200/3200 [==============================] - 0s 106us/step - loss: 5.5320e-06 - acc: 1.0000 - val_loss: 0.1178 - val_acc: 0.9875\n",
      "Epoch 61/1000\n",
      "3200/3200 [==============================] - 0s 131us/step - loss: 7.6626e-06 - acc: 1.0000 - val_loss: 0.1146 - val_acc: 0.9875\n",
      "Epoch 62/1000\n",
      "3200/3200 [==============================] - 0s 139us/step - loss: 6.3102e-06 - acc: 1.0000 - val_loss: 0.1155 - val_acc: 0.9875\n",
      "Epoch 63/1000\n",
      "3200/3200 [==============================] - 0s 121us/step - loss: 7.9733e-06 - acc: 1.0000 - val_loss: 0.1140 - val_acc: 0.9862\n",
      "Epoch 64/1000\n",
      "3200/3200 [==============================] - 0s 122us/step - loss: 4.6994e-06 - acc: 1.0000 - val_loss: 0.1195 - val_acc: 0.9875\n",
      "Epoch 65/1000\n",
      "3200/3200 [==============================] - 0s 108us/step - loss: 5.3084e-06 - acc: 1.0000 - val_loss: 0.1188 - val_acc: 0.9875\n",
      "Epoch 66/1000\n",
      "3200/3200 [==============================] - 0s 111us/step - loss: 4.6801e-06 - acc: 1.0000 - val_loss: 0.1183 - val_acc: 0.9875\n",
      "Epoch 67/1000\n",
      "3200/3200 [==============================] - 0s 112us/step - loss: 5.4923e-06 - acc: 1.0000 - val_loss: 0.1175 - val_acc: 0.9862\n",
      "Epoch 68/1000\n",
      "3200/3200 [==============================] - 0s 109us/step - loss: 5.0367e-06 - acc: 1.0000 - val_loss: 0.1190 - val_acc: 0.9862\n",
      "Epoch 69/1000\n",
      "3200/3200 [==============================] - 0s 104us/step - loss: 5.7781e-06 - acc: 1.0000 - val_loss: 0.1174 - val_acc: 0.9875\n",
      "Epoch 70/1000\n",
      "3200/3200 [==============================] - 0s 106us/step - loss: 4.4440e-06 - acc: 1.0000 - val_loss: 0.1197 - val_acc: 0.9875\n",
      "Epoch 71/1000\n",
      "3200/3200 [==============================] - 0s 123us/step - loss: 5.6932e-06 - acc: 1.0000 - val_loss: 0.1182 - val_acc: 0.9862\n",
      "Epoch 72/1000\n",
      "3200/3200 [==============================] - 0s 110us/step - loss: 3.7218e-06 - acc: 1.0000 - val_loss: 0.1205 - val_acc: 0.9875\n",
      "Epoch 73/1000\n",
      "3200/3200 [==============================] - 0s 116us/step - loss: 5.3618e-06 - acc: 1.0000 - val_loss: 0.1181 - val_acc: 0.9875\n",
      "Epoch 74/1000\n",
      "3200/3200 [==============================] - 0s 114us/step - loss: 3.8484e-06 - acc: 1.0000 - val_loss: 0.1180 - val_acc: 0.9875\n",
      "Epoch 75/1000\n",
      "3200/3200 [==============================] - 0s 109us/step - loss: 3.3068e-06 - acc: 1.0000 - val_loss: 0.1207 - val_acc: 0.9875\n",
      "Epoch 76/1000\n",
      "3200/3200 [==============================] - 0s 97us/step - loss: 3.3343e-06 - acc: 1.0000 - val_loss: 0.1213 - val_acc: 0.9875\n",
      "Epoch 77/1000\n",
      "3200/3200 [==============================] - 0s 100us/step - loss: 3.2937e-06 - acc: 1.0000 - val_loss: 0.1216 - val_acc: 0.9875\n",
      "Epoch 78/1000\n",
      "3200/3200 [==============================] - 0s 105us/step - loss: 4.1853e-06 - acc: 1.0000 - val_loss: 0.1200 - val_acc: 0.9875\n",
      "Epoch 79/1000\n",
      "3200/3200 [==============================] - 0s 104us/step - loss: 4.0254e-06 - acc: 1.0000 - val_loss: 0.1220 - val_acc: 0.9875\n",
      "Epoch 80/1000\n",
      "3200/3200 [==============================] - 0s 101us/step - loss: 3.1341e-06 - acc: 1.0000 - val_loss: 0.1235 - val_acc: 0.9875\n",
      "Epoch 81/1000\n",
      "3200/3200 [==============================] - 0s 98us/step - loss: 3.5986e-06 - acc: 1.0000 - val_loss: 0.1216 - val_acc: 0.9862\n",
      "Epoch 82/1000\n",
      "3200/3200 [==============================] - 0s 97us/step - loss: 2.6190e-06 - acc: 1.0000 - val_loss: 0.1221 - val_acc: 0.9875\n",
      "Epoch 83/1000\n",
      "3200/3200 [==============================] - 0s 111us/step - loss: 3.4258e-06 - acc: 1.0000 - val_loss: 0.1221 - val_acc: 0.9862\n",
      "Epoch 84/1000\n",
      "3200/3200 [==============================] - 0s 97us/step - loss: 2.8472e-06 - acc: 1.0000 - val_loss: 0.1220 - val_acc: 0.9875\n",
      "Epoch 85/1000\n",
      "3200/3200 [==============================] - 0s 110us/step - loss: 2.7751e-06 - acc: 1.0000 - val_loss: 0.1223 - val_acc: 0.9875\n",
      "Epoch 86/1000\n",
      "3200/3200 [==============================] - 0s 110us/step - loss: 2.2890e-06 - acc: 1.0000 - val_loss: 0.1243 - val_acc: 0.9875\n",
      "Epoch 87/1000\n",
      "3200/3200 [==============================] - 0s 117us/step - loss: 3.3629e-06 - acc: 1.0000 - val_loss: 0.1207 - val_acc: 0.9875\n",
      "Epoch 88/1000\n",
      "3200/3200 [==============================] - 0s 110us/step - loss: 2.9101e-06 - acc: 1.0000 - val_loss: 0.1196 - val_acc: 0.9875\n",
      "Epoch 89/1000\n",
      "3200/3200 [==============================] - 0s 119us/step - loss: 3.1531e-06 - acc: 1.0000 - val_loss: 0.1215 - val_acc: 0.9875\n",
      "Epoch 90/1000\n",
      "3200/3200 [==============================] - 0s 135us/step - loss: 2.8442e-06 - acc: 1.0000 - val_loss: 0.1223 - val_acc: 0.9875\n",
      "Epoch 91/1000\n",
      "3200/3200 [==============================] - 0s 135us/step - loss: 3.0969e-06 - acc: 1.0000 - val_loss: 0.1232 - val_acc: 0.9875\n",
      "Epoch 92/1000\n",
      "3200/3200 [==============================] - 0s 133us/step - loss: 2.7759e-06 - acc: 1.0000 - val_loss: 0.1228 - val_acc: 0.9862\n",
      "Epoch 93/1000\n",
      "3200/3200 [==============================] - 0s 133us/step - loss: 3.0761e-06 - acc: 1.0000 - val_loss: 0.1232 - val_acc: 0.9862\n",
      "Epoch 94/1000\n",
      "3200/3200 [==============================] - 0s 121us/step - loss: 2.1143e-06 - acc: 1.0000 - val_loss: 0.1246 - val_acc: 0.9875\n",
      "Epoch 95/1000\n",
      "3200/3200 [==============================] - 0s 137us/step - loss: 2.4837e-06 - acc: 1.0000 - val_loss: 0.1254 - val_acc: 0.9875\n",
      "Epoch 96/1000\n",
      "3200/3200 [==============================] - 0s 117us/step - loss: 1.9719e-06 - acc: 1.0000 - val_loss: 0.1253 - val_acc: 0.9875\n",
      "Epoch 97/1000\n",
      "3200/3200 [==============================] - 0s 121us/step - loss: 1.9064e-06 - acc: 1.0000 - val_loss: 0.1262 - val_acc: 0.9875\n",
      "Epoch 98/1000\n",
      "3200/3200 [==============================] - 0s 140us/step - loss: 4.2545e-06 - acc: 1.0000 - val_loss: 0.1189 - val_acc: 0.9862\n",
      "Epoch 99/1000\n",
      "3200/3200 [==============================] - 0s 132us/step - loss: 2.5226e-06 - acc: 1.0000 - val_loss: 0.1228 - val_acc: 0.9862\n",
      "Epoch 100/1000\n",
      "3200/3200 [==============================] - 0s 137us/step - loss: 1.6795e-06 - acc: 1.0000 - val_loss: 0.1263 - val_acc: 0.9862\n",
      "Epoch 101/1000\n",
      "3200/3200 [==============================] - 0s 121us/step - loss: 1.9573e-06 - acc: 1.0000 - val_loss: 0.1283 - val_acc: 0.9875\n",
      "Epoch 102/1000\n",
      "3200/3200 [==============================] - 0s 128us/step - loss: 2.0185e-06 - acc: 1.0000 - val_loss: 0.1263 - val_acc: 0.9862\n",
      "Epoch 00102: early stopping\n"
     ]
    }
   ],
   "source": [
    "#GSC CONCATENATED\n",
    "RawData, RawTarget = Select_GSC(GSC_Final, GSC_Sub_Final, Concatenated = True)\n",
    "TrainingData, TrainingTarget, ValData, ValDataAct, TestData, TestDataAct = Generate_Data_Split(RawData, RawTarget)\n",
    "model = get_model(TrainingData.shape[0])\n",
    "history = Neural_Net(TrainingData, TrainingTarget)\n",
    "\n",
    "NN_Graph(history)\n",
    "NN_Accuracy(TestData, TestDataAct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSC Dataset with feature subtraction\n",
      "(474, 5000)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 256)               121600    \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 513       \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 253,697\n",
      "Trainable params: 253,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 3200 samples, validate on 800 samples\n",
      "Epoch 1/1000\n",
      "3200/3200 [==============================] - 1s 263us/step - loss: 0.3967 - acc: 0.8225 - val_loss: 0.1557 - val_acc: 0.9425\n",
      "Epoch 2/1000\n",
      "3200/3200 [==============================] - 0s 94us/step - loss: 0.1134 - acc: 0.9619 - val_loss: 0.1010 - val_acc: 0.9600\n",
      "Epoch 3/1000\n",
      "3200/3200 [==============================] - 0s 110us/step - loss: 0.0546 - acc: 0.9844 - val_loss: 0.0890 - val_acc: 0.9613\n",
      "Epoch 4/1000\n",
      "3200/3200 [==============================] - 0s 93us/step - loss: 0.0315 - acc: 0.9919 - val_loss: 0.0957 - val_acc: 0.9663\n",
      "Epoch 5/1000\n",
      "3200/3200 [==============================] - 0s 107us/step - loss: 0.0128 - acc: 0.9984 - val_loss: 0.0784 - val_acc: 0.9675\n",
      "Epoch 6/1000\n",
      "3200/3200 [==============================] - 0s 109us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.0780 - val_acc: 0.9675\n",
      "Epoch 7/1000\n",
      "3200/3200 [==============================] - 0s 108us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 0.0759 - val_acc: 0.9712\n",
      "Epoch 8/1000\n",
      "3200/3200 [==============================] - 0s 107us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.0768 - val_acc: 0.9725\n",
      "Epoch 9/1000\n",
      "3200/3200 [==============================] - 0s 105us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0762 - val_acc: 0.9712\n",
      "Epoch 10/1000\n",
      "3200/3200 [==============================] - 0s 103us/step - loss: 9.0843e-04 - acc: 1.0000 - val_loss: 0.0765 - val_acc: 0.9725\n",
      "Epoch 11/1000\n",
      "3200/3200 [==============================] - 0s 95us/step - loss: 6.5155e-04 - acc: 1.0000 - val_loss: 0.0768 - val_acc: 0.9738\n",
      "Epoch 12/1000\n",
      "3200/3200 [==============================] - 0s 96us/step - loss: 5.6656e-04 - acc: 1.0000 - val_loss: 0.0790 - val_acc: 0.9738\n",
      "Epoch 13/1000\n",
      "3200/3200 [==============================] - 0s 98us/step - loss: 4.6177e-04 - acc: 1.0000 - val_loss: 0.0797 - val_acc: 0.9725\n",
      "Epoch 14/1000\n",
      "3200/3200 [==============================] - 0s 93us/step - loss: 3.8288e-04 - acc: 1.0000 - val_loss: 0.0804 - val_acc: 0.9738\n",
      "Epoch 15/1000\n",
      "3200/3200 [==============================] - 0s 94us/step - loss: 3.2612e-04 - acc: 1.0000 - val_loss: 0.0815 - val_acc: 0.9712\n",
      "Epoch 16/1000\n",
      "3200/3200 [==============================] - 0s 100us/step - loss: 2.7623e-04 - acc: 1.0000 - val_loss: 0.0829 - val_acc: 0.9725\n",
      "Epoch 17/1000\n",
      "3200/3200 [==============================] - 0s 104us/step - loss: 2.4460e-04 - acc: 1.0000 - val_loss: 0.0831 - val_acc: 0.9738\n",
      "Epoch 18/1000\n",
      "3200/3200 [==============================] - 0s 95us/step - loss: 2.0239e-04 - acc: 1.0000 - val_loss: 0.0830 - val_acc: 0.9712\n",
      "Epoch 19/1000\n",
      "3200/3200 [==============================] - 0s 90us/step - loss: 1.8787e-04 - acc: 1.0000 - val_loss: 0.0831 - val_acc: 0.9738\n",
      "Epoch 20/1000\n",
      "3200/3200 [==============================] - 0s 103us/step - loss: 1.7078e-04 - acc: 1.0000 - val_loss: 0.0841 - val_acc: 0.9738\n",
      "Epoch 21/1000\n",
      "3200/3200 [==============================] - 0s 96us/step - loss: 1.5424e-04 - acc: 1.0000 - val_loss: 0.0852 - val_acc: 0.9738\n",
      "Epoch 22/1000\n",
      "3200/3200 [==============================] - 0s 91us/step - loss: 1.3238e-04 - acc: 1.0000 - val_loss: 0.0851 - val_acc: 0.9725\n",
      "Epoch 23/1000\n",
      "3200/3200 [==============================] - 0s 111us/step - loss: 1.2619e-04 - acc: 1.0000 - val_loss: 0.0855 - val_acc: 0.9725\n",
      "Epoch 24/1000\n",
      "3200/3200 [==============================] - 0s 109us/step - loss: 1.1562e-04 - acc: 1.0000 - val_loss: 0.0859 - val_acc: 0.9725\n",
      "Epoch 25/1000\n",
      "3200/3200 [==============================] - 0s 87us/step - loss: 1.0211e-04 - acc: 1.0000 - val_loss: 0.0866 - val_acc: 0.9738\n",
      "Epoch 26/1000\n",
      "3200/3200 [==============================] - 0s 90us/step - loss: 9.7567e-05 - acc: 1.0000 - val_loss: 0.0874 - val_acc: 0.9725\n",
      "Epoch 27/1000\n",
      "3200/3200 [==============================] - 0s 92us/step - loss: 9.0180e-05 - acc: 1.0000 - val_loss: 0.0890 - val_acc: 0.9725\n",
      "Epoch 28/1000\n",
      "3200/3200 [==============================] - 0s 87us/step - loss: 7.8006e-05 - acc: 1.0000 - val_loss: 0.0891 - val_acc: 0.9738\n",
      "Epoch 29/1000\n",
      "3200/3200 [==============================] - 0s 108us/step - loss: 7.0689e-05 - acc: 1.0000 - val_loss: 0.0900 - val_acc: 0.9725\n",
      "Epoch 30/1000\n",
      "3200/3200 [==============================] - 0s 93us/step - loss: 6.9069e-05 - acc: 1.0000 - val_loss: 0.0897 - val_acc: 0.9738\n",
      "Epoch 31/1000\n",
      "3200/3200 [==============================] - 0s 98us/step - loss: 5.9540e-05 - acc: 1.0000 - val_loss: 0.0902 - val_acc: 0.9738\n",
      "Epoch 32/1000\n",
      "3200/3200 [==============================] - 0s 113us/step - loss: 6.0060e-05 - acc: 1.0000 - val_loss: 0.0903 - val_acc: 0.9725\n",
      "Epoch 33/1000\n",
      "3200/3200 [==============================] - 0s 101us/step - loss: 5.4110e-05 - acc: 1.0000 - val_loss: 0.0909 - val_acc: 0.9725\n",
      "Epoch 34/1000\n",
      "3200/3200 [==============================] - 0s 90us/step - loss: 5.0689e-05 - acc: 1.0000 - val_loss: 0.0915 - val_acc: 0.9738\n",
      "Epoch 35/1000\n",
      "3200/3200 [==============================] - 0s 109us/step - loss: 4.8533e-05 - acc: 1.0000 - val_loss: 0.0920 - val_acc: 0.9725\n",
      "Epoch 36/1000\n",
      "3200/3200 [==============================] - 0s 110us/step - loss: 4.4006e-05 - acc: 1.0000 - val_loss: 0.0923 - val_acc: 0.9738\n",
      "Epoch 37/1000\n",
      "3200/3200 [==============================] - 0s 110us/step - loss: 4.1522e-05 - acc: 1.0000 - val_loss: 0.0931 - val_acc: 0.9725\n",
      "Epoch 38/1000\n",
      "3200/3200 [==============================] - 0s 109us/step - loss: 3.9068e-05 - acc: 1.0000 - val_loss: 0.0933 - val_acc: 0.9738\n",
      "Epoch 39/1000\n",
      "3200/3200 [==============================] - 0s 100us/step - loss: 3.6024e-05 - acc: 1.0000 - val_loss: 0.0938 - val_acc: 0.9738\n",
      "Epoch 40/1000\n",
      "3200/3200 [==============================] - 0s 87us/step - loss: 3.3511e-05 - acc: 1.0000 - val_loss: 0.0949 - val_acc: 0.9738\n",
      "Epoch 41/1000\n",
      "3200/3200 [==============================] - 0s 98us/step - loss: 3.3320e-05 - acc: 1.0000 - val_loss: 0.0952 - val_acc: 0.9738\n",
      "Epoch 42/1000\n",
      "3200/3200 [==============================] - 0s 98us/step - loss: 2.9890e-05 - acc: 1.0000 - val_loss: 0.0953 - val_acc: 0.9738\n",
      "Epoch 43/1000\n",
      "3200/3200 [==============================] - 0s 96us/step - loss: 2.8732e-05 - acc: 1.0000 - val_loss: 0.0956 - val_acc: 0.9738\n",
      "Epoch 44/1000\n",
      "3200/3200 [==============================] - 0s 98us/step - loss: 2.8340e-05 - acc: 1.0000 - val_loss: 0.0958 - val_acc: 0.9738\n",
      "Epoch 45/1000\n",
      "3200/3200 [==============================] - 0s 100us/step - loss: 2.5718e-05 - acc: 1.0000 - val_loss: 0.0964 - val_acc: 0.9738\n",
      "Epoch 46/1000\n",
      "3200/3200 [==============================] - 0s 96us/step - loss: 2.7058e-05 - acc: 1.0000 - val_loss: 0.0969 - val_acc: 0.9738\n",
      "Epoch 47/1000\n",
      "3200/3200 [==============================] - 0s 86us/step - loss: 2.5480e-05 - acc: 1.0000 - val_loss: 0.0973 - val_acc: 0.9725\n",
      "Epoch 48/1000\n",
      "3200/3200 [==============================] - 0s 100us/step - loss: 2.1768e-05 - acc: 1.0000 - val_loss: 0.0974 - val_acc: 0.9738\n",
      "Epoch 49/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - 0s 102us/step - loss: 2.2735e-05 - acc: 1.0000 - val_loss: 0.0979 - val_acc: 0.9738\n",
      "Epoch 50/1000\n",
      "3200/3200 [==============================] - 0s 93us/step - loss: 2.1267e-05 - acc: 1.0000 - val_loss: 0.0980 - val_acc: 0.9738\n",
      "Epoch 51/1000\n",
      "3200/3200 [==============================] - 0s 88us/step - loss: 1.9535e-05 - acc: 1.0000 - val_loss: 0.0985 - val_acc: 0.9725\n",
      "Epoch 52/1000\n",
      "3200/3200 [==============================] - 0s 92us/step - loss: 1.8926e-05 - acc: 1.0000 - val_loss: 0.0985 - val_acc: 0.9738\n",
      "Epoch 53/1000\n",
      "3200/3200 [==============================] - 0s 98us/step - loss: 1.7124e-05 - acc: 1.0000 - val_loss: 0.0987 - val_acc: 0.9738\n",
      "Epoch 54/1000\n",
      "3200/3200 [==============================] - 0s 95us/step - loss: 1.7592e-05 - acc: 1.0000 - val_loss: 0.0988 - val_acc: 0.9738\n",
      "Epoch 55/1000\n",
      "3200/3200 [==============================] - 0s 119us/step - loss: 1.6096e-05 - acc: 1.0000 - val_loss: 0.0992 - val_acc: 0.9738\n",
      "Epoch 56/1000\n",
      "3200/3200 [==============================] - 0s 120us/step - loss: 1.6195e-05 - acc: 1.0000 - val_loss: 0.0996 - val_acc: 0.9738\n",
      "Epoch 57/1000\n",
      "3200/3200 [==============================] - 0s 127us/step - loss: 1.4880e-05 - acc: 1.0000 - val_loss: 0.1001 - val_acc: 0.9738\n",
      "Epoch 58/1000\n",
      "3200/3200 [==============================] - 0s 131us/step - loss: 1.3553e-05 - acc: 1.0000 - val_loss: 0.1006 - val_acc: 0.9738\n",
      "Epoch 59/1000\n",
      "3200/3200 [==============================] - 0s 105us/step - loss: 1.4593e-05 - acc: 1.0000 - val_loss: 0.1009 - val_acc: 0.9738\n",
      "Epoch 60/1000\n",
      "3200/3200 [==============================] - 0s 118us/step - loss: 1.4264e-05 - acc: 1.0000 - val_loss: 0.1011 - val_acc: 0.9738\n",
      "Epoch 61/1000\n",
      "3200/3200 [==============================] - 0s 97us/step - loss: 1.4885e-05 - acc: 1.0000 - val_loss: 0.1016 - val_acc: 0.9738\n",
      "Epoch 62/1000\n",
      "3200/3200 [==============================] - 0s 95us/step - loss: 1.2505e-05 - acc: 1.0000 - val_loss: 0.1020 - val_acc: 0.9738\n",
      "Epoch 63/1000\n",
      "3200/3200 [==============================] - 0s 96us/step - loss: 1.1814e-05 - acc: 1.0000 - val_loss: 0.1025 - val_acc: 0.9738\n",
      "Epoch 64/1000\n",
      "3200/3200 [==============================] - 0s 90us/step - loss: 1.1410e-05 - acc: 1.0000 - val_loss: 0.1027 - val_acc: 0.9738\n",
      "Epoch 65/1000\n",
      "3200/3200 [==============================] - 0s 122us/step - loss: 1.0510e-05 - acc: 1.0000 - val_loss: 0.1030 - val_acc: 0.9738\n",
      "Epoch 66/1000\n",
      "3200/3200 [==============================] - 0s 113us/step - loss: 1.0732e-05 - acc: 1.0000 - val_loss: 0.1031 - val_acc: 0.9738\n",
      "Epoch 67/1000\n",
      "3200/3200 [==============================] - 0s 114us/step - loss: 1.0077e-05 - acc: 1.0000 - val_loss: 0.1031 - val_acc: 0.9738\n",
      "Epoch 68/1000\n",
      "3200/3200 [==============================] - 0s 112us/step - loss: 9.8945e-06 - acc: 1.0000 - val_loss: 0.1033 - val_acc: 0.9738\n",
      "Epoch 69/1000\n",
      "3200/3200 [==============================] - 0s 102us/step - loss: 9.4656e-06 - acc: 1.0000 - val_loss: 0.1037 - val_acc: 0.9738\n",
      "Epoch 70/1000\n",
      "3200/3200 [==============================] - 0s 103us/step - loss: 8.8865e-06 - acc: 1.0000 - val_loss: 0.1043 - val_acc: 0.9738\n",
      "Epoch 71/1000\n",
      "3200/3200 [==============================] - 0s 101us/step - loss: 9.3586e-06 - acc: 1.0000 - val_loss: 0.1047 - val_acc: 0.9738\n",
      "Epoch 72/1000\n",
      "3200/3200 [==============================] - 0s 85us/step - loss: 8.1606e-06 - acc: 1.0000 - val_loss: 0.1049 - val_acc: 0.9738\n",
      "Epoch 73/1000\n",
      "3200/3200 [==============================] - 0s 84us/step - loss: 8.7472e-06 - acc: 1.0000 - val_loss: 0.1050 - val_acc: 0.9738\n",
      "Epoch 74/1000\n",
      "3200/3200 [==============================] - 0s 85us/step - loss: 7.7613e-06 - acc: 1.0000 - val_loss: 0.1050 - val_acc: 0.9738\n",
      "Epoch 75/1000\n",
      "3200/3200 [==============================] - 0s 89us/step - loss: 7.4064e-06 - acc: 1.0000 - val_loss: 0.1055 - val_acc: 0.9738\n",
      "Epoch 76/1000\n",
      "3200/3200 [==============================] - 0s 83us/step - loss: 8.1038e-06 - acc: 1.0000 - val_loss: 0.1054 - val_acc: 0.9738\n",
      "Epoch 77/1000\n",
      "3200/3200 [==============================] - 0s 81us/step - loss: 6.7062e-06 - acc: 1.0000 - val_loss: 0.1056 - val_acc: 0.9738\n",
      "Epoch 78/1000\n",
      "3200/3200 [==============================] - 0s 83us/step - loss: 6.9603e-06 - acc: 1.0000 - val_loss: 0.1059 - val_acc: 0.9738\n",
      "Epoch 79/1000\n",
      "3200/3200 [==============================] - 0s 83us/step - loss: 7.6820e-06 - acc: 1.0000 - val_loss: 0.1062 - val_acc: 0.9738\n",
      "Epoch 80/1000\n",
      "3200/3200 [==============================] - 0s 85us/step - loss: 6.7600e-06 - acc: 1.0000 - val_loss: 0.1062 - val_acc: 0.9738\n",
      "Epoch 81/1000\n",
      "3200/3200 [==============================] - 0s 90us/step - loss: 7.5460e-06 - acc: 1.0000 - val_loss: 0.1062 - val_acc: 0.9738\n",
      "Epoch 82/1000\n",
      "3200/3200 [==============================] - 0s 92us/step - loss: 6.7426e-06 - acc: 1.0000 - val_loss: 0.1065 - val_acc: 0.9738\n",
      "Epoch 83/1000\n",
      "3200/3200 [==============================] - 0s 93us/step - loss: 6.1153e-06 - acc: 1.0000 - val_loss: 0.1068 - val_acc: 0.9738\n",
      "Epoch 84/1000\n",
      "3200/3200 [==============================] - 0s 92us/step - loss: 6.4072e-06 - acc: 1.0000 - val_loss: 0.1071 - val_acc: 0.9738\n",
      "Epoch 85/1000\n",
      "3200/3200 [==============================] - 0s 87us/step - loss: 5.5768e-06 - acc: 1.0000 - val_loss: 0.1072 - val_acc: 0.9738\n",
      "Epoch 86/1000\n",
      "3200/3200 [==============================] - 0s 99us/step - loss: 6.6694e-06 - acc: 1.0000 - val_loss: 0.1075 - val_acc: 0.9738\n",
      "Epoch 87/1000\n",
      "3200/3200 [==============================] - 0s 91us/step - loss: 5.7120e-06 - acc: 1.0000 - val_loss: 0.1078 - val_acc: 0.9738\n",
      "Epoch 88/1000\n",
      "3200/3200 [==============================] - 0s 91us/step - loss: 5.6867e-06 - acc: 1.0000 - val_loss: 0.1082 - val_acc: 0.9738\n",
      "Epoch 89/1000\n",
      "3200/3200 [==============================] - 0s 102us/step - loss: 5.5032e-06 - acc: 1.0000 - val_loss: 0.1084 - val_acc: 0.9738\n",
      "Epoch 90/1000\n",
      "3200/3200 [==============================] - 0s 103us/step - loss: 5.4175e-06 - acc: 1.0000 - val_loss: 0.1087 - val_acc: 0.9738\n",
      "Epoch 91/1000\n",
      "3200/3200 [==============================] - 0s 100us/step - loss: 5.9540e-06 - acc: 1.0000 - val_loss: 0.1086 - val_acc: 0.9738\n",
      "Epoch 92/1000\n",
      "3200/3200 [==============================] - 0s 106us/step - loss: 5.6391e-06 - acc: 1.0000 - val_loss: 0.1088 - val_acc: 0.9738\n",
      "Epoch 93/1000\n",
      "3200/3200 [==============================] - 0s 107us/step - loss: 5.4284e-06 - acc: 1.0000 - val_loss: 0.1089 - val_acc: 0.9738\n",
      "Epoch 94/1000\n",
      "3200/3200 [==============================] - 0s 107us/step - loss: 4.7285e-06 - acc: 1.0000 - val_loss: 0.1088 - val_acc: 0.9738\n",
      "Epoch 95/1000\n",
      "3200/3200 [==============================] - 0s 101us/step - loss: 4.3903e-06 - acc: 1.0000 - val_loss: 0.1090 - val_acc: 0.9738\n",
      "Epoch 96/1000\n",
      "3200/3200 [==============================] - 0s 88us/step - loss: 4.5711e-06 - acc: 1.0000 - val_loss: 0.1091 - val_acc: 0.9738\n",
      "Epoch 97/1000\n",
      "3200/3200 [==============================] - 0s 106us/step - loss: 4.4908e-06 - acc: 1.0000 - val_loss: 0.1095 - val_acc: 0.9738\n",
      "Epoch 98/1000\n",
      "3200/3200 [==============================] - 0s 109us/step - loss: 4.3140e-06 - acc: 1.0000 - val_loss: 0.1095 - val_acc: 0.9738\n",
      "Epoch 99/1000\n",
      "3200/3200 [==============================] - 0s 121us/step - loss: 4.1670e-06 - acc: 1.0000 - val_loss: 0.1096 - val_acc: 0.9738\n",
      "Epoch 100/1000\n",
      "3200/3200 [==============================] - 0s 118us/step - loss: 4.8345e-06 - acc: 1.0000 - val_loss: 0.1095 - val_acc: 0.9738\n",
      "Epoch 101/1000\n",
      "3200/3200 [==============================] - 0s 113us/step - loss: 3.8483e-06 - acc: 1.0000 - val_loss: 0.1097 - val_acc: 0.9738\n",
      "Epoch 102/1000\n",
      "3200/3200 [==============================] - 0s 96us/step - loss: 3.7448e-06 - acc: 1.0000 - val_loss: 0.1100 - val_acc: 0.9738\n",
      "Epoch 103/1000\n",
      "3200/3200 [==============================] - 0s 114us/step - loss: 4.4926e-06 - acc: 1.0000 - val_loss: 0.1102 - val_acc: 0.9738\n",
      "Epoch 104/1000\n",
      "3200/3200 [==============================] - 0s 95us/step - loss: 3.8434e-06 - acc: 1.0000 - val_loss: 0.1104 - val_acc: 0.9738\n",
      "Epoch 105/1000\n",
      "3200/3200 [==============================] - 0s 114us/step - loss: 3.7769e-06 - acc: 1.0000 - val_loss: 0.1107 - val_acc: 0.9738\n",
      "Epoch 106/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3200/3200 [==============================] - 0s 110us/step - loss: 3.6286e-06 - acc: 1.0000 - val_loss: 0.1109 - val_acc: 0.9738\n",
      "Epoch 107/1000\n",
      "3200/3200 [==============================] - 0s 85us/step - loss: 3.6810e-06 - acc: 1.0000 - val_loss: 0.1108 - val_acc: 0.9738\n",
      "Epoch 00107: early stopping\n"
     ]
    }
   ],
   "source": [
    "#GSC CONCATENATED\n",
    "RawData, RawTarget = Select_GSC(GSC_Final, GSC_Sub_Final, Concatenated = False)\n",
    "TrainingData, TrainingTarget, ValData, ValDataAct, TestData, TestDataAct = Generate_Data_Split(RawData, RawTarget)\n",
    "model = get_model(TrainingData.shape[0])\n",
    "history = Neural_Net(TrainingData, TrainingTarget)\n",
    "\n",
    "NN_Graph(history)\n",
    "NN_Accuracy(TestData, TestDataAct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): \n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def loss(h, y):\n",
    "    return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "\n",
    "def GetAccuracy(W, ValData, ValDataAct):\n",
    "    final_score = np.dot(ValData.T, W.T)\n",
    "    #print(final_score.shape)\n",
    "\n",
    "    preds = np.round(sigmoid(final_score))\n",
    "    #print(preds[1])\n",
    "\n",
    "    right = 0\n",
    "    for i in range(ValDataAct.shape[0]):\n",
    "        if preds[i] == ValDataAct[i]:\n",
    "            right = right + 1\n",
    "    #print(right)\n",
    "\n",
    "    return (right/ValDataAct.shape[0])\n",
    "\n",
    "def Logistic_Regression(TrainingData, TrainingTarget, ValData, ValDataAct, TestData, TestDataAct):\n",
    "    W = np.zeros((1,TrainingData.shape[0]))\n",
    "    LR = 0.2\n",
    "\n",
    "    for i in range(10000):\n",
    "        Z = np.dot(W, TrainingData)\n",
    "        #print(Z.shape)\n",
    "\n",
    "        y_cap = sigmoid(Z)\n",
    "        #print(y_cap)\n",
    "\n",
    "        delta_E = np.dot((y_cap - TrainingTarget), TrainingData.T)\n",
    "        #print(delta_E.shape)\n",
    "\n",
    "        #Update weights\n",
    "        W = W -  np.dot(LR, delta_E)\n",
    "\n",
    "        #if i%1000 == 0:\n",
    "           # print(\"Loss %f\"  %loss(y_cap,TrainingTarget))\n",
    "    print(W.shape)\n",
    "\n",
    "    Training_acc = GetAccuracy(W, TrainingData, TrainingTarget)\n",
    "    Validation_acc = GetAccuracy(W, ValData, ValDataAct)\n",
    "    Testing_acc = GetAccuracy(W, TestData, TestDataAct)\n",
    "    #Unseen_acc = GetAccuracy(W, Unseen_GSC_Final[:, :1024], Unseen_GSC_Final[:, 1024:])\n",
    "    print(\"Accuracy measures:\")\n",
    "    print(\"Training data accuracy\", Training_acc)\n",
    "    print(\"Validation data accuracy\", Validation_acc)\n",
    "    print(\"Test data accuracy\", Testing_acc)\n",
    "\n",
    "    #JUST TO VERIFY THE ACCURACY USING SKLEARN\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    clf = LogisticRegression(fit_intercept=True, C = 1e15)\n",
    "    clf.fit(TrainingData.T, TrainingTarget)\n",
    "    print ('Accuracy from sk-learn on training:', clf.score(TrainingData.T, TrainingTarget))\n",
    "    print ('Accuracy from sk-learn on validation:', clf.score(ValData.T, ValDataAct))\n",
    "    print ('Accuracy from sk-learn on testing:', clf.score(TestData.T, TestDataAct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Observed Dataset with Concatenation selected\n",
      "(1582, 18)\n",
      "(1, 18)\n",
      "Accuracy measures:\n",
      "Training data accuracy 0.9549763033175356\n",
      "Validation data accuracy 0.9367088607594937\n",
      "Test data accuracy 0.9554140127388535\n",
      "Accuracy from sk-learn on training: 0.9605055292259084\n",
      "Accuracy from sk-learn on validation: 0.9430379746835443\n",
      "Accuracy from sk-learn on testing: 0.9490445859872612\n"
     ]
    }
   ],
   "source": [
    "#HUMAN OBSERVED DATASET CONCATENATED\n",
    "RawData, RawTarget = Select_HODS(Final_HODS, Final_SHODS, Concatenated = True)\n",
    "#print(RawData.shape, RawTarget.shape)\n",
    "TrainingData, TrainingTarget, ValData, ValDataAct, TestData, TestDataAct = Generate_Data_Split(RawData, RawTarget)\n",
    "Logistic_Regression(TrainingData, TrainingTarget, ValData, ValDataAct, TestData, TestDataAct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Observed Dataset with Subtraction selected\n",
      "(1, 9)\n",
      "Accuracy measures:\n",
      "Training data accuracy 0.6216429699842022\n",
      "Validation data accuracy 0.620253164556962\n",
      "Test data accuracy 0.6751592356687898\n",
      "Accuracy from sk-learn on training: 0.8199052132701422\n",
      "Accuracy from sk-learn on validation: 0.7784810126582279\n",
      "Accuracy from sk-learn on testing: 0.8535031847133758\n"
     ]
    }
   ],
   "source": [
    "#HUMAN OBSERVED DATASET SUBTRACTED\n",
    "RawData, RawTarget = Select_HODS(Final_HODS, Final_SHODS, Concatenated = False)\n",
    "#print(RawData.shape, RawTarget.shape)\n",
    "TrainingData, TrainingTarget, ValData, ValDataAct, TestData, TestDataAct = Generate_Data_Split(RawData, RawTarget)\n",
    "Logistic_Regression(TrainingData, TrainingTarget, ValData, ValDataAct, TestData, TestDataAct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSC Dataset with feature concatenation\n",
      "(940, 5000)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-832e3b5542c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m                                )\n\u001b[0;32m      4\u001b[0m \u001b[0mTrainingData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrainingTarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValDataAct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTestData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTestDataAct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGenerate_Data_Split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRawData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRawTarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mLogistic_Regression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTrainingData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrainingTarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValDataAct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTestData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTestDataAct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-62-ae00bbfdcfb8>\u001b[0m in \u001b[0;36mLogistic_Regression\u001b[1;34m(TrainingData, TrainingTarget, ValData, ValDataAct, TestData, TestDataAct)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrainingData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;31m#print(Z.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#GSC CONCATENATED\n",
    "RawData, RawTarget = Select_GSC(GSC_Final, GSC_Sub_Final, Concatenated = True)\n",
    "TrainingData, TrainingTarget, ValData, ValDataAct, TestData, TestDataAct = Generate_Data_Split(RawData, RawTarget)\n",
    "Logistic_Regression(TrainingData, TrainingTarget, ValData, ValDataAct, TestData, TestDataAct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GSC SUBTRACTED\n",
    "RawData, RawTarget = Select_GSC(GSC_Final, GSC_Sub_Final, Concatenated = False)\n",
    "TrainingData, TrainingTarget, ValData, ValDataAct, TestData, TestDataAct = Generate_Data_Split(RawData, RawTarget)\n",
    "Logistic_Regression(TrainingData, TrainingTarget, ValData, ValDataAct, TestData, TestDataAct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
